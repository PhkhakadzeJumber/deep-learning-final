{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289},{"sourceId":1483651,"sourceType":"datasetVersion","datasetId":870709}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==========================================\n# 1. SETUP & IMPORTS\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.amp import autocast, GradScaler\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\nimport torchvision.models as models\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport math\nimport re\nimport shutil\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom collections import defaultdict","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-24T12:43:39.973205Z","iopub.execute_input":"2026-01-24T12:43:39.973785Z","iopub.status.idle":"2026-01-24T12:43:49.403427Z","shell.execute_reply.started":"2026-01-24T12:43:39.973761Z","shell.execute_reply":"2026-01-24T12:43:49.402809Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# BEST PRACTICE: Set seeds for reproducibility\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n# Device configuration (GPU if available, else CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T12:43:49.405113Z","iopub.execute_input":"2026-01-24T12:43:49.405454Z","iopub.status.idle":"2026-01-24T12:43:49.482340Z","shell.execute_reply.started":"2026-01-24T12:43:49.405430Z","shell.execute_reply":"2026-01-24T12:43:49.481332Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"file_path = '/kaggle/input/flickr8k/captions.txt'\n\n# read file\nimg_caption_pairs = []\n\nwith open(file_path, 'r', encoding='utf-8') as f:\n    lines = [line.strip() for line in f if line.strip()]\n\n# Remove header\nlines = lines[1:]\n\nfor line in lines:\n    img, caption = line.split(',', 1)\n    img_caption_pairs.append((img, caption.lower()))\n\nprint(\"First (image, caption) pair:\")\nprint(img_caption_pairs[0])\n\n# save only captions for tokenizer\ncaptions_file = '/kaggle/working/captions_clean.txt'\n\nwith open(captions_file, 'w', encoding='utf-8') as f:\n    for _, caption in img_caption_pairs:\n        f.write(caption + '\\n')\n\ndef tokenize(caption):\n    return re.findall(r\"\\w+\", caption.lower())\n\nSPECIAL_TOKENS = {\n    \"<pad>\": 0,\n    \"<unk>\": 1,\n    \"<bos>\": 2,\n    \"<eos>\": 3\n}\n\ndef build_vocab(captions, min_freq=2):\n    counter = Counter()\n    for c in captions:\n        counter.update(tokenize(c))\n\n    vocab = dict(SPECIAL_TOKENS)\n    idx = len(vocab)\n\n    for word, freq in counter.items():\n        if freq >= min_freq:\n            vocab[word] = idx\n            idx += 1\n\n    return vocab\n\ndef load_glove_embeddings(glove_path, vocab, embed_dim=300):\n    embeddings = np.random.normal(\n        scale=0.6,\n        size=(len(vocab), embed_dim)\n    )\n\n    found = 0\n    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            if word in vocab:\n                vector = np.asarray(values[1:], dtype=\"float32\")\n                embeddings[vocab[word]] = vector\n                found += 1\n\n    print(f\"Loaded GloVe vectors for {found}/{len(vocab)} words\")\n    return torch.tensor(embeddings, dtype=torch.float32)\n\nall_captions = [c for _, c in img_caption_pairs]\nvocab = build_vocab(all_captions, min_freq=2)\n\nprint(\"Vocab size:\", len(vocab))\n\nglove_path = \"/kaggle/input/glove-embeddings/glove.6B.300d.txt\"\nglove_embeddings = load_glove_embeddings(glove_path, vocab)\ntorch.save(glove_embeddings, '/kaggle/working/glove_embeddings.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T12:43:49.483157Z","iopub.execute_input":"2026-01-24T12:43:49.483376Z","iopub.status.idle":"2026-01-24T12:44:03.097785Z","shell.execute_reply.started":"2026-01-24T12:43:49.483353Z","shell.execute_reply":"2026-01-24T12:44:03.097085Z"}},"outputs":[{"name":"stdout","text":"First (image, caption) pair:\n('1000268201_693b08cb0e.jpg', 'a child in a pink dress is climbing up a set of stairs in an entry way .')\nVocab size: 5156\nLoaded GloVe vectors for 5089/5156 words\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from PIL import Image\nimport torchvision.transforms as transforms\n\nclass ImageCaptionDataset(Dataset):\n    def __init__(self, img_caption_pairs, vocab, image_root, transform=None):\n        self.data = img_caption_pairs\n        self.vocab = vocab\n        self.image_root = image_root\n        self.transform = transform\n\n        self.pad_id = vocab[\"<pad>\"]\n        self.unk_id = vocab[\"<unk>\"]\n        self.bos_id = vocab[\"<bos>\"]\n        self.eos_id = vocab[\"<eos>\"]\n\n    def encode_caption(self, caption):\n        tokens = tokenize(caption)\n        ids = [self.vocab.get(t, self.unk_id) for t in tokens]\n        return [self.bos_id] + ids + [self.eos_id]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name, caption = self.data[idx]\n\n        image = Image.open(f\"{self.image_root}/{img_name}\").convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n\n        caption_ids = self.encode_caption(caption)\n        caption_tensor = torch.tensor(caption_ids, dtype=torch.long)\n\n        return image, caption_tensor, len(caption_tensor)\n\nimage_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T12:44:03.098642Z","iopub.execute_input":"2026-01-24T12:44:03.098872Z","iopub.status.idle":"2026-01-24T12:44:03.107090Z","shell.execute_reply.started":"2026-01-24T12:44:03.098850Z","shell.execute_reply":"2026-01-24T12:44:03.106439Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def collate_fn(batch):\n    images, captions, lengths = zip(*batch)\n\n    images = torch.stack(images)\n    captions = pad_sequence(\n        captions,\n        batch_first=True,\n        padding_value=vocab[\"<pad>\"]\n    )\n\n    return images, captions, torch.tensor(lengths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T12:44:03.107883Z","iopub.execute_input":"2026-01-24T12:44:03.108142Z","iopub.status.idle":"2026-01-24T12:44:03.128053Z","shell.execute_reply.started":"2026-01-24T12:44:03.108104Z","shell.execute_reply":"2026-01-24T12:44:03.127514Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# train/val split\n\nimg_to_captions = defaultdict(list)\n\nfor img, caption in img_caption_pairs:\n  img_to_captions[img].append(caption)\n\nall_images = list(img_to_captions.keys())\n\ntrain_images, val_images = train_test_split(\n    all_images,\n    test_size=0.2,\n    random_state=SEED\n)\n\ntrain_pairs = []\nval_pairs = []\n\nfor img in train_images:\n  for caption in img_to_captions[img]:\n    train_pairs.append((img, caption))\n\nfor img in val_images:\n  for caption in img_to_captions[img]:\n    val_pairs.append((img, caption))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T12:44:03.128896Z","iopub.execute_input":"2026-01-24T12:44:03.129110Z","iopub.status.idle":"2026-01-24T12:44:03.165044Z","shell.execute_reply.started":"2026-01-24T12:44:03.129091Z","shell.execute_reply":"2026-01-24T12:44:03.164515Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"image_root = '/kaggle/input/flickr8k/Images'\n\ntrain_dataset = ImageCaptionDataset(train_pairs, vocab, image_root, transform=image_transform)\nval_dataset   = ImageCaptionDataset(val_pairs, vocab, image_root, transform=image_transform)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True,\n    prefetch_factor=4,\n    collate_fn=collate_fn,\n    persistent_workers=True   # keeps workers alive between epochs\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=64,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True,\n    collate_fn=collate_fn,\n    persistent_workers=True\n)\n\nimages, captions, lengths = next(iter(train_loader))\n\nprint(\"Images:\", images.shape)        # (B, 3, 256, 256)\nprint(\"Captions:\", captions.shape)    # (B, max_len)\nprint(\"Lengths:\", lengths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T12:44:03.166636Z","iopub.execute_input":"2026-01-24T12:44:03.166853Z","iopub.status.idle":"2026-01-24T12:44:04.638807Z","shell.execute_reply.started":"2026-01-24T12:44:03.166833Z","shell.execute_reply":"2026-01-24T12:44:04.637613Z"}},"outputs":[{"name":"stdout","text":"Images: torch.Size([64, 3, 256, 256])\nCaptions: torch.Size([64, 21])\nLengths: tensor([16,  8, 12,  9, 16, 16, 13,  9, 11, 15,  8, 11, 11, 11, 19, 15, 10, 18,\n        15, 21, 10,  9, 16, 19, 11, 10, 16, 14, 13,  9, 11, 14, 21, 10, 11, 14,\n        15, 16,  7, 18, 18,  9, 11, 13, 14, 13, 10, 18, 11,  8, 14,  9, 14, 14,\n        15,  8, 12, 11, 20, 17, 17, 12, 13, 11])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"class ImgToCaptionModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim=512, max_seq_len=50, pad_token_id=0):\n        super().__init__()\n\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        self.max_seq_len = max_seq_len\n        self.pad_token_id = pad_token_id\n\n        # 2D positional embeddings for image features\n        self.row_embed = nn.Parameter(torch.randn(8, embed_dim) * 0.02)\n        self.col_embed = nn.Parameter(torch.randn(8, embed_dim) * 0.02)\n        \n        # CNN ENCODER\n        resnet = models.resnet50(pretrained=True)\n        self.cnn = nn.Sequential(*list(resnet.children())[:-2])\n\n        # Project visual features\n        self.prep = nn.Sequential(\n            nn.Linear(2048, 300),\n            nn.LayerNorm(300),\n            nn.ReLU(),\n            nn.Dropout(0.1)\n        )\n\n        # TEXT EMBEDDING\n        self.embedding = nn.Embedding.from_pretrained(\n            glove_embeddings,\n            freeze=True,\n            padding_idx=pad_token_id\n        )\n        \n        self.embed_dropout = nn.Dropout(0.1)\n\n        # Positional encoding for captions \n        self.pos_encoding = nn.Parameter(\n            torch.randn(max_seq_len, embed_dim) * 0.02\n        )\n\n        # TRANSFORMER DECODER\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=300,\n            nhead=6,              # IMPORTANT: 300 % 6 == 0\n            dim_feedforward=1024,\n            batch_first=True\n        )\n\n        self.transformer_decoder = nn.TransformerDecoder(\n            decoder_layer,\n            num_layers=3\n        )\n\n        self.fc_out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, images, captions):\n        # IMAGE ENCODING\n        img_features = self.cnn(images)               # (B, 2048, 8, 8)\n        img_features = img_features.flatten(2)        # (B, 2048, 64)\n        img_features = img_features.transpose(1, 2)   # (B, 64, 2048)\n        img_features = self.prep(img_features)        # (B, 64, 512)                                \n    \n        # Add 2D positional encoding to image features\n        pos = self.row_embed[:, None, :] + self.col_embed[None, :, :]\n        pos = pos.reshape(64, -1)\n        img_features = img_features + pos.unsqueeze(0)\n        \n        # TEXT EMBEDDING\n        seq_len = captions.size(1)\n        caption_embeds = self.embedding(captions)\n        caption_embeds = self.embed_dropout(caption_embeds)\n        caption_embeds = caption_embeds + self.pos_encoding[:seq_len]\n\n        # MASKS\n        tgt_mask = torch.triu(\n            torch.ones(seq_len, seq_len, device=captions.device, dtype=torch.bool),\n            diagonal=1\n        )\n\n        tgt_key_padding_mask = (captions == self.pad_token_id)\n        \n        # DECODER\n        output = self.transformer_decoder(\n            tgt=caption_embeds,\n            memory=img_features,\n            tgt_mask=tgt_mask,\n            tgt_key_padding_mask=tgt_key_padding_mask\n        )\n\n        output = self.fc_out(output)\n\n        return output\n\n# Initialize the model\nmodel = ImgToCaptionModel(\n    vocab_size=len(vocab),\n    embed_dim=300,\n    max_seq_len=50,\n    pad_token_id=vocab[\"<pad>\"]\n).to(device)\n\nmodel = model.to(device)\n\n# number of total params\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total Parameters: {total_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T12:44:04.642333Z","iopub.execute_input":"2026-01-24T12:44:04.643078Z","iopub.status.idle":"2026-01-24T12:44:06.309998Z","shell.execute_reply.started":"2026-01-24T12:44:04.642972Z","shell.execute_reply":"2026-01-24T12:44:06.309073Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 145MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Total Parameters: 31,261,660\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# perplexity metric calculation (clearer indicator than just raw loss function)\ndef perplexity_from_loss(loss):\n    return math.exp(loss) if loss < 20 else float(\"inf\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T12:44:06.310986Z","iopub.execute_input":"2026-01-24T12:44:06.311357Z","iopub.status.idle":"2026-01-24T12:44:06.316415Z","shell.execute_reply.started":"2026-01-24T12:44:06.311334Z","shell.execute_reply":"2026-01-24T12:44:06.315345Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"num_epochs = 15                   \nlearning_rate = 1e-4  \n\npad_token_id = 0\nstart_token_id = 2  \nend_token_id = 3\n\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_token_id, label_smoothing=0.05)\n\n# Separate CNN and other parameters for different learning rates\ncnn_params = list(model.cnn.parameters())\nother_params = [p for n, p in model.named_parameters() if not n.startswith(\"cnn\")]\n\noptimizer = optim.AdamW([\n    {'params': cnn_params, 'lr': 1e-5},        # slow LR for pretrained CNN\n    {'params': other_params, 'lr': 1e-4}       # higher LR for transformer & prep layers\n], weight_decay=1e-4)\n\n# Learning rate scheduler \nscheduler = ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=0.5,\n    patience=3,\n    min_lr=1e-6\n)\n\n# Mixed precision scaler\nscaler = GradScaler()\n\n# For saving best model\nbest_val_loss = float('inf')\n\n# Training loop\nfor epoch in range(num_epochs):\n    # ========== TRAIN ==========\n    model.train()\n    train_loss = 0.0\n\n    for images, captions, lengths in train_loader:\n        images = images.to(device)\n        captions = captions.to(device)\n\n        inputs = captions[:, :-1]\n        targets = captions[:, 1:]\n\n        optimizer.zero_grad()\n\n        with autocast(device_type='cuda'):\n            outputs = model(images, inputs)\n            loss = criterion(\n                outputs.reshape(-1, outputs.size(-1)),\n                targets.reshape(-1)\n            )\n\n        # Backprop with AMP\n        scaler.scale(loss).backward()\n\n        # Unscale before clipping\n        scaler.unscale_(optimizer)\n        clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_loss += loss.item()\n\n    avg_train_loss = train_loss / len(train_loader)\n    train_ppl = perplexity_from_loss(avg_train_loss)\n\n    # ========== VALIDATION ==========\n    model.eval()\n    val_loss = 0.0\n    bleu_scores = []\n\n    with torch.no_grad():\n        for images, captions, lengths in val_loader:\n            images = images.to(device)\n            captions = captions.to(device)\n\n            inputs = captions[:, :-1]\n            targets = captions[:, 1:]\n\n            with autocast(device_type='cuda'):\n                outputs = model(images, inputs)\n                loss = criterion(\n                    outputs.reshape(-1, outputs.size(-1)),\n                    targets.reshape(-1)\n                )\n\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(val_loader)\n    val_ppl = perplexity_from_loss(avg_val_loss)\n\n    scheduler.step(avg_val_loss)\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        torch.save(model.state_dict(), '/kaggle/working/image_caption_model_cnnfrozen_best.pth')\n        print(\"âœ… Best model saved!\")\n\n    print(f\"\\nðŸ“Š Epoch [{epoch+1}/{num_epochs}]\")\n    print(f\"   Train Loss: {avg_train_loss:.4f}\")\n    print(f\"   Train PPL:  {train_ppl:.2f}\")\n    print(f\"   Val Loss:   {avg_val_loss:.4f}\")\n    print(f\"   Val PPL:    {val_ppl:.2f}\")\n    print(f\"   LR (CNN):      {optimizer.param_groups[0]['lr']:.6f}\")\n    print(f\"   LR (Transformer): {optimizer.param_groups[1]['lr']:.6f}\")\n\nprint(\"\\nâœ… Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T12:44:06.317624Z","iopub.execute_input":"2026-01-24T12:44:06.317951Z","iopub.status.idle":"2026-01-24T13:15:25.276236Z","shell.execute_reply.started":"2026-01-24T12:44:06.317917Z","shell.execute_reply":"2026-01-24T13:15:25.275241Z"}},"outputs":[{"name":"stdout","text":"âœ… Best model saved!\n\nðŸ“Š Epoch [1/15]\n   Train Loss: 4.7184\n   Train PPL:  111.99\n   Val Loss:   4.0327\n   Val PPL:    56.41\n   LR (CNN):      0.000010\n   LR (Transformer): 0.000100\nâœ… Best model saved!\n\nðŸ“Š Epoch [2/15]\n   Train Loss: 3.8730\n   Train PPL:  48.08\n   Val Loss:   3.7273\n   Val PPL:    41.56\n   LR (CNN):      0.000010\n   LR (Transformer): 0.000100\nâœ… Best model saved!\n\nðŸ“Š Epoch [3/15]\n   Train Loss: 3.6124\n   Train PPL:  37.06\n   Val Loss:   3.5654\n   Val PPL:    35.36\n   LR (CNN):      0.000010\n   LR (Transformer): 0.000100\nâœ… Best model saved!\n\nðŸ“Š Epoch [4/15]\n   Train Loss: 3.4461\n   Train PPL:  31.38\n   Val Loss:   3.4749\n   Val PPL:    32.29\n   LR (CNN):      0.000010\n   LR (Transformer): 0.000100\nâœ… Best model saved!\n\nðŸ“Š Epoch [5/15]\n   Train Loss: 3.3214\n   Train PPL:  27.70\n   Val Loss:   3.3995\n   Val PPL:    29.95\n   LR (CNN):      0.000010\n   LR (Transformer): 0.000100\nâœ… Best model saved!\n\nðŸ“Š Epoch [6/15]\n   Train Loss: 3.2215\n   Train PPL:  25.07\n   Val Loss:   3.3607\n   Val PPL:    28.81\n   LR (CNN):      0.000010\n   LR (Transformer): 0.000100\nâœ… Best model saved!\n\nðŸ“Š Epoch [7/15]\n   Train Loss: 3.1363\n   Train PPL:  23.02\n   Val Loss:   3.3273\n   Val PPL:    27.86\n   LR (CNN):      0.000010\n   LR (Transformer): 0.000100\nâœ… Best model saved!\n\nðŸ“Š Epoch [8/15]\n   Train Loss: 3.0624\n   Train PPL:  21.38\n   Val Loss:   3.2977\n   Val PPL:    27.05\n   LR (CNN):      0.000010\n   LR (Transformer): 0.000100\nâœ… Best model saved!\n\nðŸ“Š Epoch [9/15]\n   Train Loss: 2.9942\n   Train PPL:  19.97\n   Val Loss:   3.2847\n   Val PPL:    26.70\n   LR (CNN):      0.000010\n   LR (Transformer): 0.000100\nâœ… Best model saved!\n\nðŸ“Š Epoch [10/15]\n   Train Loss: 2.9337\n   Train PPL:  18.80\n   Val Loss:   3.2683\n   Val PPL:    26.27\n   LR (CNN):      0.000010\n   LR (Transformer): 0.000100\nâœ… Best model saved!\n\nðŸ“Š Epoch [11/15]\n   Train Loss: 2.8758\n   Train PPL:  17.74\n   Val Loss:   3.2597\n   Val PPL:    26.04\n   LR (CNN):      0.000010\n   LR (Transformer): 0.000100\nâœ… Best model saved!\n\nðŸ“Š Epoch [12/15]\n   Train Loss: 2.8254\n   Train PPL:  16.87\n   Val Loss:   3.2595\n   Val PPL:    26.04\n   LR (CNN):      0.000010\n   LR (Transformer): 0.000100\nâœ… Best model saved!\n\nðŸ“Š Epoch [13/15]\n   Train Loss: 2.7747\n   Train PPL:  16.03\n   Val Loss:   3.2480\n   Val PPL:    25.74\n   LR (CNN):      0.000010\n   LR (Transformer): 0.000100\n\nðŸ“Š Epoch [14/15]\n   Train Loss: 2.7276\n   Train PPL:  15.30\n   Val Loss:   3.2489\n   Val PPL:    25.76\n   LR (CNN):      0.000010\n   LR (Transformer): 0.000100\n\nðŸ“Š Epoch [15/15]\n   Train Loss: 2.6838\n   Train PPL:  14.64\n   Val Loss:   3.2493\n   Val PPL:    25.77\n   LR (CNN):      0.000010\n   LR (Transformer): 0.000100\n\nâœ… Training complete!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import shutil\n\n# Save model\ntorch.save(model.state_dict(), '/kaggle/working/image_caption_model_cnnfrozen_last.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T13:15:25.278013Z","iopub.execute_input":"2026-01-24T13:15:25.278738Z","iopub.status.idle":"2026-01-24T13:15:25.435168Z","shell.execute_reply.started":"2026-01-24T13:15:25.278689Z","shell.execute_reply":"2026-01-24T13:15:25.434360Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import pickle\n\nwith open('/kaggle/working/vocab.pkl', 'wb') as f:\n    pickle.dump(vocab, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T13:15:25.436173Z","iopub.execute_input":"2026-01-24T13:15:25.436420Z","iopub.status.idle":"2026-01-24T13:15:25.441996Z","shell.execute_reply.started":"2026-01-24T13:15:25.436396Z","shell.execute_reply":"2026-01-24T13:15:25.441319Z"}},"outputs":[],"execution_count":12}]}