{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb5174c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-23T16:03:35.231823Z",
     "iopub.status.busy": "2026-01-23T16:03:35.231568Z",
     "iopub.status.idle": "2026-01-23T16:03:46.243718Z",
     "shell.execute_reply": "2026-01-23T16:03:46.242727Z"
    },
    "papermill": {
     "duration": 11.017408,
     "end_time": "2026-01-23T16:03:46.245452",
     "exception": false,
     "start_time": "2026-01-23T16:03:35.228044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. SETUP & IMPORTS\n",
    "# ==========================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "import torchvision.models as models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7300e084",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T16:03:46.250780Z",
     "iopub.status.busy": "2026-01-23T16:03:46.250106Z",
     "iopub.status.idle": "2026-01-23T16:03:46.307639Z",
     "shell.execute_reply": "2026-01-23T16:03:46.306877Z"
    },
    "papermill": {
     "duration": 0.061624,
     "end_time": "2026-01-23T16:03:46.309159",
     "exception": false,
     "start_time": "2026-01-23T16:03:46.247535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# BEST PRACTICE: Set seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23afe5d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T16:03:46.314264Z",
     "iopub.status.busy": "2026-01-23T16:03:46.313717Z",
     "iopub.status.idle": "2026-01-23T16:03:47.126736Z",
     "shell.execute_reply": "2026-01-23T16:03:47.125898Z"
    },
    "papermill": {
     "duration": 0.818247,
     "end_time": "2026-01-23T16:03:47.129309",
     "exception": false,
     "start_time": "2026-01-23T16:03:46.311062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First (image, caption) pair:\n",
      "('1000268201_693b08cb0e.jpg', 'a child in a pink dress is climbing up a set of stairs in an entry way .')\n",
      "Vocabulary size: 10000\n",
      "Special tokens:\n",
      "{'<pad>': 0, '<unk>': 1, '<s>': 2, '</s>': 3}\n",
      "\n",
      "First caption:\n",
      "a child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "\n",
      "Subword tokens:\n",
      "['‚ñÅa', '‚ñÅchild', '‚ñÅin', '‚ñÅa', '‚ñÅpink', '‚ñÅdress', '‚ñÅis', '‚ñÅclimbing', '‚ñÅup', '‚ñÅa', '‚ñÅset', '‚ñÅof', '‚ñÅstairs', '‚ñÅin', '‚ñÅan', '‚ñÅentry', '‚ñÅway', '‚ñÅ.']\n",
      "\n",
      "Subword token IDs:\n",
      "[4, 128, 15, 4, 325, 270, 40, 414, 207, 4, 719, 46, 1045, 15, 135, 9136, 1603, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /kaggle/working/captions_clean.txt\n",
      "  input_format: \n",
      "  model_prefix: /kaggle/working/spm\n",
      "  model_type: BPE\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: /kaggle/working/captions_clean.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 40455 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=2276235\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9679% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=31\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999679\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 40455 sentences.\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 40455\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 9130\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=88342 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15511 size=20 all=1052 active=1020 piece=‚ñÅc\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7958 size=40 all=1637 active=1605 piece=‚ñÅr\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5087 size=60 all=2134 active=2102 piece=and\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3960 size=80 all=2658 active=2626 piece=ut\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3232 size=100 all=3002 active=2970 piece=ur\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3232 min_freq=83\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2758 size=120 all=3366 active=1301 piece=oo\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2447 size=140 all=3814 active=1749 piece=‚ñÅho\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2096 size=160 all=4227 active=2162 piece=‚ñÅsk\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1786 size=180 all=4448 active=2383 piece=‚ñÅstanding\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1487 size=200 all=4739 active=2674 piece=‚ñÅpo\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1477 min_freq=93\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1272 size=220 all=4931 active=1170 piece=‚ñÅthree\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1167 size=240 all=5189 active=1428 piece=‚ñÅstre\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1034 size=260 all=5378 active=1617 piece=‚ñÅmouth\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=945 size=280 all=5593 active=1832 piece=‚ñÅits\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=831 size=300 all=5752 active=1991 piece=‚ñÅmountain\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=830 min_freq=86\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=742 size=320 all=5951 active=1195 piece=‚ñÅorange\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=667 size=340 all=6159 active=1403 piece=ak\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=610 size=360 all=6394 active=1638 piece=one\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=575 size=380 all=6572 active=1816 piece=‚ñÅbic\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=528 size=400 all=6839 active=2083 piece=ps\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=526 min_freq=77\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=487 size=420 all=7062 active=1188 piece=lasses\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=459 size=440 all=7179 active=1305 piece=‚ñÅcarr\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=430 size=460 all=7382 active=1508 piece=nis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=412 size=480 all=7489 active=1615 piece=‚ñÅhelmet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=385 size=500 all=7618 active=1744 piece=‚ñÅrace\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=384 min_freq=68\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=359 size=520 all=7727 active=1109 piece=‚ñÅbase\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=341 size=540 all=7911 active=1293 piece=‚ñÅget\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=317 size=560 all=8057 active=1439 piece=are\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=304 size=580 all=8248 active=1630 piece=‚ñÅtre\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=288 size=600 all=8405 active=1787 piece=‚ñÅplayers\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=286 min_freq=60\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=266 size=620 all=8500 active=1096 piece=und\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=253 size=640 all=8617 active=1213 piece=‚ñÅaf\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=245 size=660 all=8789 active=1385 piece=‚ñÅam\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=233 size=680 all=8928 active=1524 piece=dd\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=224 size=700 all=9108 active=1704 piece=‚ñÅice\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=224 min_freq=53\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=211 size=720 all=9175 active=1066 piece=‚ñÅcollar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=201 size=740 all=9227 active=1118 piece=‚ñÅyard\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=193 size=760 all=9262 active=1153 piece=ort\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=182 size=780 all=9379 active=1270 piece=‚ñÅtodd\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=171 size=800 all=9500 active=1391 piece=ep\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=171 min_freq=49\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=164 size=820 all=9575 active=1061 piece=ican\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=158 size=840 all=9690 active=1176 piece=‚ñÅcr\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=153 size=860 all=9739 active=1225 piece=‚ñÅnearby\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=145 size=880 all=9818 active=1304 piece=‚ñÅwin\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=139 size=900 all=9877 active=1363 piece=‚ñÅbott\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=138 min_freq=43\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=133 size=920 all=9952 active=1072 piece=‚ñÅhouse\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=129 size=940 all=10043 active=1163 piece=‚ñÅsplashing\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=125 size=960 all=10110 active=1230 piece=‚ñÅobstacle\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=120 size=980 all=10168 active=1288 piece=‚ñÅdis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=118 size=1000 all=10211 active=1331 piece=erman\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=118 min_freq=39\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=113 size=1020 all=10277 active=1063 piece=‚ñÅmale\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=109 size=1040 all=10379 active=1165 piece=‚ñÅkis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=104 size=1060 all=10405 active=1191 piece=‚ñÅcart\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=100 size=1080 all=10527 active=1313 piece=els\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=98 size=1100 all=10583 active=1369 piece=‚ñÅvest\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=98 min_freq=35\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=95 size=1120 all=10624 active=1042 piece=ins\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=93 size=1140 all=10710 active=1128 piece=‚ñÅslo\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=90 size=1160 all=10766 active=1184 piece=‚ñÅenjo\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=87 size=1180 all=10814 active=1232 piece=‚ñÅfeet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=85 size=1200 all=10917 active=1335 piece=‚ñÅcle\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=85 min_freq=32\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=83 size=1220 all=10969 active=1045 piece=ike\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=81 size=1240 all=11016 active=1092 piece=‚ñÅbare\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=79 size=1260 all=11034 active=1110 piece=‚ñÅtank\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=76 size=1280 all=11087 active=1163 piece=ome\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=74 size=1300 all=11150 active=1226 piece=‚ñÅring\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=74 min_freq=28\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=73 size=1320 all=11175 active=1024 piece=‚ñÅinflatable\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=71 size=1340 all=11264 active=1113 piece=‚ñÅmic\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=69 size=1360 all=11329 active=1178 piece=pes\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=68 size=1380 all=11430 active=1279 piece=‚ñÅjackets\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=66 size=1400 all=11447 active=1296 piece=mo\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=66 min_freq=26\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=65 size=1420 all=11524 active=1053 piece=‚ñÅscoot\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=63 size=1440 all=11591 active=1120 piece=‚ñÅcut\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=61 size=1460 all=11620 active=1149 piece=too\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=60 size=1480 all=11661 active=1190 piece=‚ñÅscooter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58 size=1500 all=11718 active=1247 piece=‚ñÅbut\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=58 min_freq=24\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=57 size=1520 all=11769 active=1047 piece=‚ñÅbeaut\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=1540 all=11819 active=1097 piece=eling\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: f"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "file_path = '/kaggle/input/flickr8k/captions.txt'\n",
    "\n",
    "# read file\n",
    "img_caption_pairs = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Remove header\n",
    "lines = lines[1:]\n",
    "\n",
    "for line in lines:\n",
    "    img, caption = line.split(',', 1)\n",
    "    img_caption_pairs.append((img, caption.lower()))\n",
    "\n",
    "print(\"First (image, caption) pair:\")\n",
    "print(img_caption_pairs[0])\n",
    "\n",
    "# save only captions for tokenizer\n",
    "captions_file = '/kaggle/working/captions_clean.txt'\n",
    "\n",
    "with open(captions_file, 'w', encoding='utf-8') as f:\n",
    "    for _, caption in img_caption_pairs:\n",
    "        f.write(caption + '\\n')\n",
    "\n",
    "# train tokenizer\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=captions_file,\n",
    "    model_prefix='/kaggle/working/spm',\n",
    "    vocab_size=10000,\n",
    "    model_type='bpe',\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3\n",
    ")\n",
    "\n",
    "# load tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('/kaggle/working/spm.model')\n",
    "\n",
    "# building vocabulary\n",
    "vocab = {sp.id_to_piece(i): i for i in range(sp.get_piece_size())}\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"Special tokens:\")\n",
    "print({k: v for k, v in vocab.items() if k in [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]})\n",
    "\n",
    "# Example: subword tokenization of first caption\n",
    "first_caption = img_caption_pairs[0][1]\n",
    "\n",
    "subword_tokens = sp.encode(first_caption, out_type=str)\n",
    "subword_ids = sp.encode(first_caption, out_type=int)\n",
    "\n",
    "print(\"\\nFirst caption:\")\n",
    "print(first_caption)\n",
    "\n",
    "print(\"\\nSubword tokens:\")\n",
    "print(subword_tokens)\n",
    "\n",
    "print(\"\\nSubword token IDs:\")\n",
    "print(subword_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe597e51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T16:03:47.135263Z",
     "iopub.status.busy": "2026-01-23T16:03:47.134823Z",
     "iopub.status.idle": "2026-01-23T16:03:47.141471Z",
     "shell.execute_reply": "2026-01-23T16:03:47.140832Z"
    },
    "papermill": {
     "duration": 0.011087,
     "end_time": "2026-01-23T16:03:47.142901",
     "exception": false,
     "start_time": "2026-01-23T16:03:47.131814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, img_caption_pairs, sp, image_root, transform=None):\n",
    "        self.data = img_caption_pairs\n",
    "        self.sp = sp # tokenizer\n",
    "        self.image_root = image_root # path where the images are\n",
    "        self.transform = transform\n",
    "\n",
    "        self.bos_id = sp.bos_id()\n",
    "        self.eos_id = sp.eos_id()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, caption = self.data[idx]\n",
    "\n",
    "        # ---- Load image ----\n",
    "        img_path = f\"{self.image_root}/{img_name}\"\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # ---- Tokenize caption ----\n",
    "        caption_ids = self.sp.encode(caption, out_type=int)\n",
    "\n",
    "        # Add <bos> and <eos>\n",
    "        caption_ids = [self.bos_id] + caption_ids + [self.eos_id]\n",
    "\n",
    "        caption_tensor = torch.tensor(caption_ids, dtype=torch.long)\n",
    "\n",
    "        return image, caption_tensor, len(caption_tensor)\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97e7b2c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T16:03:47.148356Z",
     "iopub.status.busy": "2026-01-23T16:03:47.147834Z",
     "iopub.status.idle": "2026-01-23T16:03:47.152242Z",
     "shell.execute_reply": "2026-01-23T16:03:47.151582Z"
    },
    "papermill": {
     "duration": 0.008631,
     "end_time": "2026-01-23T16:03:47.153651",
     "exception": false,
     "start_time": "2026-01-23T16:03:47.145020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, captions, lengths = zip(*batch)\n",
    "\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    captions_padded = pad_sequence(\n",
    "        captions,\n",
    "        batch_first=True,\n",
    "        padding_value=sp.pad_id()\n",
    "    )\n",
    "\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    return images, captions_padded, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c38dcb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T16:03:47.158895Z",
     "iopub.status.busy": "2026-01-23T16:03:47.158582Z",
     "iopub.status.idle": "2026-01-23T16:03:47.185877Z",
     "shell.execute_reply": "2026-01-23T16:03:47.185170Z"
    },
    "papermill": {
     "duration": 0.031555,
     "end_time": "2026-01-23T16:03:47.187281",
     "exception": false,
     "start_time": "2026-01-23T16:03:47.155726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train/val split\n",
    "\n",
    "img_to_captions = defaultdict(list)\n",
    "\n",
    "for img, caption in img_caption_pairs:\n",
    "  img_to_captions[img].append(caption)\n",
    "\n",
    "all_images = list(img_to_captions.keys())\n",
    "\n",
    "train_images, val_images = train_test_split(\n",
    "    all_images,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "train_pairs = []\n",
    "val_pairs = []\n",
    "\n",
    "for img in train_images:\n",
    "  for caption in img_to_captions[img]:\n",
    "    train_pairs.append((img, caption))\n",
    "\n",
    "for img in val_images:\n",
    "  for caption in img_to_captions[img]:\n",
    "    val_pairs.append((img, caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c04759",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T16:03:47.192796Z",
     "iopub.status.busy": "2026-01-23T16:03:47.192221Z",
     "iopub.status.idle": "2026-01-23T16:03:48.911200Z",
     "shell.execute_reply": "2026-01-23T16:03:48.909666Z"
    },
    "papermill": {
     "duration": 1.724659,
     "end_time": "2026-01-23T16:03:48.914049",
     "exception": false,
     "start_time": "2026-01-23T16:03:47.189390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: torch.Size([64, 3, 256, 256])\n",
      "Captions: torch.Size([64, 24])\n",
      "Lengths: tensor([17,  9, 13, 10, 17, 17, 14,  9, 12, 16,  9, 12, 12, 12, 20, 16, 11, 20,\n",
      "        16, 22, 11, 10, 19, 21, 12, 11, 20, 17, 14, 10, 12, 15, 24, 10, 12, 14,\n",
      "        16, 17,  7, 21, 19, 10, 12, 14, 15, 14, 11, 20, 12,  9, 15,  9, 15, 15,\n",
      "        18,  9, 13, 12, 21, 18, 18, 13, 14, 12])\n"
     ]
    }
   ],
   "source": [
    "image_root = '/kaggle/input/flickr8k/Images'\n",
    "\n",
    "train_dataset = ImageCaptionDataset(train_pairs, sp, image_root, transform=image_transform)\n",
    "val_dataset   = ImageCaptionDataset(val_pairs, sp, image_root, transform=image_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    collate_fn=collate_fn,\n",
    "    persistent_workers=True   # keeps workers alive between epochs\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "images, captions, lengths = next(iter(train_loader))\n",
    "\n",
    "print(\"Images:\", images.shape)        # (B, 3, 256, 256)\n",
    "print(\"Captions:\", captions.shape)    # (B, max_len)\n",
    "print(\"Lengths:\", lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e0d474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T16:03:48.924074Z",
     "iopub.status.busy": "2026-01-23T16:03:48.923767Z",
     "iopub.status.idle": "2026-01-23T16:03:51.155059Z",
     "shell.execute_reply": "2026-01-23T16:03:51.154300Z"
    },
    "papermill": {
     "duration": 2.237379,
     "end_time": "2026-01-23T16:03:51.156809",
     "exception": false,
     "start_time": "2026-01-23T16:03:48.919430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 157MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 44,305,232\n"
     ]
    }
   ],
   "source": [
    "class ImgToCaptionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, max_seq_len=50, pad_token_id=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        # 2D positional embeddings for image features\n",
    "        self.row_embed = nn.Parameter(torch.randn(8, embed_dim) * 0.02)\n",
    "        self.col_embed = nn.Parameter(torch.randn(8, embed_dim) * 0.02)\n",
    "        \n",
    "        # CNN ENCODER\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-2])  # (B, 2048, 8, 8)\n",
    "\n",
    "        # Project visual features\n",
    "        self.prep = nn.Sequential(\n",
    "            nn.Linear(2048, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        # TEXT EMBEDDING\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embed_dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Positional encoding for captions \n",
    "        self.pos_encoding = nn.Parameter(\n",
    "            torch.randn(max_seq_len, embed_dim) * 0.02\n",
    "        )\n",
    "\n",
    "        # TRANSFORMER DECODER\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=1024,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=3\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        # IMAGE ENCODING\n",
    "        img_features = self.cnn(images)               # (B, 2048, 8, 8)\n",
    "        img_features = img_features.flatten(2)        # (B, 2048, 64)\n",
    "        img_features = img_features.transpose(1, 2)   # (B, 64, 2048)\n",
    "        img_features = self.prep(img_features)        # (B, 64, 512)                                \n",
    "    \n",
    "        # Add 2D positional encoding to image features\n",
    "        pos = self.row_embed[:, None, :] + self.col_embed[None, :, :]\n",
    "        pos = pos.reshape(64, -1)\n",
    "        img_features = img_features + pos.unsqueeze(0)\n",
    "        \n",
    "        # TEXT EMBEDDING\n",
    "        seq_len = captions.size(1)\n",
    "        caption_embeds = self.embedding(captions)\n",
    "        caption_embeds = self.embed_dropout(caption_embeds)\n",
    "        caption_embeds = caption_embeds + self.pos_encoding[:seq_len]\n",
    "\n",
    "        # MASKS\n",
    "        tgt_mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=captions.device, dtype=torch.bool),\n",
    "            diagonal=1\n",
    "        )\n",
    "\n",
    "        tgt_key_padding_mask = (captions == self.pad_token_id)\n",
    "        \n",
    "        # DECODER\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=caption_embeds,\n",
    "            memory=img_features,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Initialize the model \n",
    "vocab_size = 10000 \n",
    "model = ImgToCaptionModel(\n",
    "    vocab_size=vocab_size, \n",
    "    embed_dim=512, \n",
    "    max_seq_len=50,  \n",
    "    pad_token_id=0\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# number of total params\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed764e44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T16:03:51.168486Z",
     "iopub.status.busy": "2026-01-23T16:03:51.168197Z",
     "iopub.status.idle": "2026-01-23T16:03:51.172357Z",
     "shell.execute_reply": "2026-01-23T16:03:51.171525Z"
    },
    "papermill": {
     "duration": 0.01337,
     "end_time": "2026-01-23T16:03:51.174847",
     "exception": false,
     "start_time": "2026-01-23T16:03:51.161477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perplexity metric calculation (clearer indicator than just raw loss function)\n",
    "def perplexity_from_loss(loss):\n",
    "    return math.exp(loss) if loss < 20 else float(\"inf\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61dd5a17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T16:03:51.186327Z",
     "iopub.status.busy": "2026-01-23T16:03:51.185775Z",
     "iopub.status.idle": "2026-01-23T17:06:21.608434Z",
     "shell.execute_reply": "2026-01-23T17:06:21.607419Z"
    },
    "papermill": {
     "duration": 3750.433441,
     "end_time": "2026-01-23T17:06:21.612760",
     "exception": false,
     "start_time": "2026-01-23T16:03:51.179319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Best model saved!\n",
      "\n",
      "üìä Epoch [1/15]\n",
      "   Train Loss: 4.4488\n",
      "   Train PPL:  85.52\n",
      "   Val Loss:   3.7991\n",
      "   Val PPL:    44.66\n",
      "   LR (CNN):      0.000010\n",
      "   LR (Transformer): 0.000100\n",
      "‚úÖ Best model saved!\n",
      "\n",
      "üìä Epoch [2/15]\n",
      "   Train Loss: 3.5899\n",
      "   Train PPL:  36.23\n",
      "   Val Loss:   3.5414\n",
      "   Val PPL:    34.52\n",
      "   LR (CNN):      0.000010\n",
      "   LR (Transformer): 0.000100\n",
      "‚úÖ Best model saved!\n",
      "\n",
      "üìä Epoch [3/15]\n",
      "   Train Loss: 3.2964\n",
      "   Train PPL:  27.02\n",
      "   Val Loss:   3.4189\n",
      "   Val PPL:    30.54\n",
      "   LR (CNN):      0.000010\n",
      "   LR (Transformer): 0.000100\n",
      "‚úÖ Best model saved!\n",
      "\n",
      "üìä Epoch [4/15]\n",
      "   Train Loss: 3.0960\n",
      "   Train PPL:  22.11\n",
      "   Val Loss:   3.3584\n",
      "   Val PPL:    28.74\n",
      "   LR (CNN):      0.000010\n",
      "   LR (Transformer): 0.000100\n",
      "‚úÖ Best model saved!\n",
      "\n",
      "üìä Epoch [5/15]\n",
      "   Train Loss: 2.9398\n",
      "   Train PPL:  18.91\n",
      "   Val Loss:   3.3234\n",
      "   Val PPL:    27.76\n",
      "   LR (CNN):      0.000010\n",
      "   LR (Transformer): 0.000100\n",
      "‚úÖ Best model saved!\n",
      "\n",
      "üìä Epoch [6/15]\n",
      "   Train Loss: 2.8099\n",
      "   Train PPL:  16.61\n",
      "   Val Loss:   3.3145\n",
      "   Val PPL:    27.51\n",
      "   LR (CNN):      0.000010\n",
      "   LR (Transformer): 0.000100\n",
      "‚úÖ Best model saved!\n",
      "\n",
      "üìä Epoch [7/15]\n",
      "   Train Loss: 2.6965\n",
      "   Train PPL:  14.83\n",
      "   Val Loss:   3.3070\n",
      "   Val PPL:    27.30\n",
      "   LR (CNN):      0.000010\n",
      "   LR (Transformer): 0.000100\n",
      "\n",
      "üìä Epoch [8/15]\n",
      "   Train Loss: 2.5925\n",
      "   Train PPL:  13.36\n",
      "   Val Loss:   3.3109\n",
      "   Val PPL:    27.41\n",
      "   LR (CNN):      0.000010\n",
      "   LR (Transformer): 0.000100\n",
      "\n",
      "üìä Epoch [9/15]\n",
      "   Train Loss: 2.4984\n",
      "   Train PPL:  12.16\n",
      "   Val Loss:   3.3137\n",
      "   Val PPL:    27.49\n",
      "   LR (CNN):      0.000010\n",
      "   LR (Transformer): 0.000100\n",
      "\n",
      "üìä Epoch [10/15]\n",
      "   Train Loss: 2.4103\n",
      "   Train PPL:  11.14\n",
      "   Val Loss:   3.3271\n",
      "   Val PPL:    27.86\n",
      "   LR (CNN):      0.000010\n",
      "   LR (Transformer): 0.000100\n",
      "\n",
      "üìä Epoch [11/15]\n",
      "   Train Loss: 2.3271\n",
      "   Train PPL:  10.25\n",
      "   Val Loss:   3.3478\n",
      "   Val PPL:    28.44\n",
      "   LR (CNN):      0.000005\n",
      "   LR (Transformer): 0.000050\n",
      "\n",
      "üìä Epoch [12/15]\n",
      "   Train Loss: 2.1997\n",
      "   Train PPL:  9.02\n",
      "   Val Loss:   3.3662\n",
      "   Val PPL:    28.97\n",
      "   LR (CNN):      0.000005\n",
      "   LR (Transformer): 0.000050\n",
      "\n",
      "üìä Epoch [13/15]\n",
      "   Train Loss: 2.1496\n",
      "   Train PPL:  8.58\n",
      "   Val Loss:   3.3822\n",
      "   Val PPL:    29.44\n",
      "   LR (CNN):      0.000005\n",
      "   LR (Transformer): 0.000050\n",
      "\n",
      "üìä Epoch [14/15]\n",
      "   Train Loss: 2.1062\n",
      "   Train PPL:  8.22\n",
      "   Val Loss:   3.3975\n",
      "   Val PPL:    29.89\n",
      "   LR (CNN):      0.000005\n",
      "   LR (Transformer): 0.000050\n",
      "\n",
      "üìä Epoch [15/15]\n",
      "   Train Loss: 2.0643\n",
      "   Train PPL:  7.88\n",
      "   Val Loss:   3.4170\n",
      "   Val PPL:    30.48\n",
      "   LR (CNN):      0.000003\n",
      "   LR (Transformer): 0.000025\n",
      "\n",
      "‚úÖ Training complete!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15                   \n",
    "learning_rate = 1e-4  \n",
    "\n",
    "pad_token_id = 0\n",
    "start_token_id = 2  \n",
    "end_token_id = 3\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id, label_smoothing=0.05)\n",
    "\n",
    "# Separate CNN and other parameters for different learning rates\n",
    "cnn_params = list(model.cnn.parameters())\n",
    "other_params = [p for n, p in model.named_parameters() if not n.startswith(\"cnn\")]\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': cnn_params, 'lr': 1e-5},        # slow LR for pretrained CNN\n",
    "    {'params': other_params, 'lr': 1e-4}       # higher LR for transformer & prep layers\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler \n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# For saving best model\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # ========== TRAIN ==========\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, captions, lengths in train_loader:\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        inputs = captions[:, :-1]\n",
    "        targets = captions[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(images, inputs)\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, outputs.size(-1)),\n",
    "                targets.reshape(-1)\n",
    "            )\n",
    "\n",
    "        # Backprop with AMP\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscale before clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_ppl = perplexity_from_loss(avg_train_loss)\n",
    "\n",
    "    # ========== VALIDATION ==========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    bleu_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in val_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            inputs = captions[:, :-1]\n",
    "            targets = captions[:, 1:]\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(images, inputs)\n",
    "                loss = criterion(\n",
    "                    outputs.reshape(-1, outputs.size(-1)),\n",
    "                    targets.reshape(-1)\n",
    "                )\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_ppl = perplexity_from_loss(avg_val_loss)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), '/kaggle/working/image_caption_model.pth')\n",
    "        print(\"‚úÖ Best model saved!\")\n",
    "\n",
    "    print(f\"\\nüìä Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"   Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"   Train PPL:  {train_ppl:.2f}\")\n",
    "    print(f\"   Val Loss:   {avg_val_loss:.4f}\")\n",
    "    print(f\"   Val PPL:    {val_ppl:.2f}\")\n",
    "    print(f\"   LR (CNN):      {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(f\"   LR (Transformer): {optimizer.param_groups[1]['lr']:.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 623289,
     "sourceId": 1111676,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3771.693826,
   "end_time": "2026-01-23T17:06:24.437510",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-23T16:03:32.743684",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
