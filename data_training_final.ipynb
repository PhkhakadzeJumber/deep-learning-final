{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhkhakadzeJumber/deep-learning-final/blob/main/data_training_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w5STuNd1sfp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b638c1a-8f10-439d-b4dc-67a3af6d8598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Go to notebook folder\n",
        "#cd /content/drive/MyDrive/Colab\\ Notebooks/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a local folder in Colab VM\n",
        "%mkdir -p /content/my_repo\n",
        "%cd /content/my_repo\n",
        "\n",
        "# Clone your GitHub repo directly (recommended)\n",
        "!git clone https://github.com/PhkhakadzeJumber/deep-learning-final.git .\n"
      ],
      "metadata": {
        "id": "8WCw_HOTkRub",
        "outputId": "ffb4d7f5-5002-4faa-fbb6-93bcbde144a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/my_repo\n",
            "Cloning into '.'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. SETUP & IMPORTS\n",
        "# ==========================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "duajBSSlU62X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BEST PRACTICE: Set seeds for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Device configuration (GPU if available, else CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "ajoSWjYUVHzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4e72464-4067-4639-b18d-1fa3245d0f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "file_path = '/content/drive/MyDrive/caption_data/captions.txt'\n",
        "\n",
        "# read file\n",
        "img_caption_pairs = []\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    lines = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# Remove header\n",
        "lines = lines[1:]\n",
        "\n",
        "for line in lines:\n",
        "    img, caption = line.split(',', 1)\n",
        "    img_caption_pairs.append((img, caption.lower()))\n",
        "\n",
        "print(\"First (image, caption) pair:\")\n",
        "print(img_caption_pairs[0])\n",
        "\n",
        "# save only captions for tokenizer\n",
        "captions_file = '/content/captions_clean.txt'\n",
        "\n",
        "with open(captions_file, 'w', encoding='utf-8') as f:\n",
        "    for _, caption in img_caption_pairs:\n",
        "        f.write(caption + '\\n')\n",
        "\n",
        "# train tokenizer\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=captions_file,\n",
        "    model_prefix='/content/spm',\n",
        "    vocab_size=8000,\n",
        "    model_type='bpe',\n",
        "    pad_id=0,\n",
        "    unk_id=1,\n",
        "    bos_id=2,\n",
        "    eos_id=3\n",
        ")\n",
        "\n",
        "# load tokenizer\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('/content/spm.model')\n",
        "\n",
        "# building vocabulary\n",
        "vocab = {sp.id_to_piece(i): i for i in range(sp.get_piece_size())}\n",
        "\n",
        "print(\"Vocabulary size:\", len(vocab))\n",
        "print(\"Special tokens:\")\n",
        "print({k: v for k, v in vocab.items() if k in [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]})\n",
        "\n",
        "# Example: subword tokenization of first caption\n",
        "first_caption = img_caption_pairs[0][1]\n",
        "\n",
        "subword_tokens = sp.encode(first_caption, out_type=str)\n",
        "subword_ids = sp.encode(first_caption, out_type=int)\n",
        "\n",
        "print(\"\\nFirst caption:\")\n",
        "print(first_caption)\n",
        "\n",
        "print(\"\\nSubword tokens:\")\n",
        "print(subword_tokens)\n",
        "\n",
        "print(\"\\nSubword token IDs:\")\n",
        "print(subword_ids)\n"
      ],
      "metadata": {
        "id": "XJ7s2RjTVTLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92e9b5d6-2f56-4143-9970-89a2f1097cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First (image, caption) pair:\n",
            "('1000268201_693b08cb0e.jpg', 'a child in a pink dress is climbing up a set of stairs in an entry way .')\n",
            "Vocabulary size: 8000\n",
            "Special tokens:\n",
            "{'<pad>': 0, '<unk>': 1, '<s>': 2, '</s>': 3}\n",
            "\n",
            "First caption:\n",
            "a child in a pink dress is climbing up a set of stairs in an entry way .\n",
            "\n",
            "Subword tokens:\n",
            "['▁a', '▁child', '▁in', '▁a', '▁pink', '▁dress', '▁is', '▁climbing', '▁up', '▁a', '▁set', '▁of', '▁stairs', '▁in', '▁an', '▁ent', 'ry', '▁way', '▁.']\n",
            "\n",
            "Subword token IDs:\n",
            "[4, 128, 15, 4, 325, 270, 40, 414, 207, 4, 719, 46, 1045, 15, 135, 1879, 715, 1603, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, img_caption_pairs, sp, image_root, transform=None):\n",
        "        self.data = img_caption_pairs\n",
        "        self.sp = sp # tokenizer\n",
        "        self.image_root = image_root # path where the images are\n",
        "        self.transform = transform\n",
        "\n",
        "        self.bos_id = sp.bos_id()\n",
        "        self.eos_id = sp.eos_id()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, caption = self.data[idx]\n",
        "\n",
        "        # ---- Load image ----\n",
        "        img_path = f\"{self.image_root}/{img_name}\"\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # ---- Tokenize caption ----\n",
        "        caption_ids = self.sp.encode(caption, out_type=int)\n",
        "\n",
        "        # Add <bos> and <eos>\n",
        "        caption_ids = [self.bos_id] + caption_ids + [self.eos_id]\n",
        "\n",
        "        caption_tensor = torch.tensor(caption_ids, dtype=torch.long)\n",
        "\n",
        "        return image, caption_tensor, len(caption_tensor)\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "1c6eABVw22eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    images, captions, lengths = zip(*batch)\n",
        "\n",
        "    images = torch.stack(images, dim=0)\n",
        "\n",
        "    captions_padded = pad_sequence(\n",
        "        captions,\n",
        "        batch_first=True,\n",
        "        padding_value=sp.pad_id()\n",
        "    )\n",
        "\n",
        "    lengths = torch.tensor(lengths)\n",
        "\n",
        "    return images, captions_padded, lengths\n"
      ],
      "metadata": {
        "id": "6m6IKtb64QUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train/val split\n",
        "\n",
        "img_to_captions = defaultdict(list)\n",
        "\n",
        "for img, caption in img_caption_pairs:\n",
        "  img_to_captions[img].append(caption)\n",
        "\n",
        "all_images = list(img_to_captions.keys())\n",
        "\n",
        "train_images, val_images = train_test_split(\n",
        "    all_images,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "train_pairs = []\n",
        "val_pairs = []\n",
        "\n",
        "for img in train_images:\n",
        "  for caption in img_to_captions[img]:\n",
        "    train_pairs.append((img, caption))\n",
        "\n",
        "for img in val_images:\n",
        "  for caption in img_to_captions[img]:\n",
        "    val_pairs.append((img, caption))"
      ],
      "metadata": {
        "id": "idB5QOhp5LHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_root = '/content/drive/MyDrive/caption_data/Images'\n",
        "\n",
        "train_dataset = ImageCaptionDataset(train_pairs, sp, image_root, transform=image_transform)\n",
        "val_dataset   = ImageCaptionDataset(val_pairs, sp, image_root, transform=image_transform)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "images, captions, lengths = next(iter(train_loader))\n",
        "\n",
        "print(\"Images:\", images.shape)        # (B, 3, 256, 256)\n",
        "print(\"Captions:\", captions.shape)    # (B, max_len)\n",
        "print(\"Lengths:\", lengths)"
      ],
      "metadata": {
        "id": "a92f40QiaNi4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dec3090e-c522-4d1a-be1e-ba538b5ae920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images: torch.Size([64, 3, 256, 256])\n",
            "Captions: torch.Size([64, 24])\n",
            "Lengths: tensor([17,  9, 13, 10, 17, 17, 14,  9, 12, 16,  9, 12, 12, 12, 20, 16, 11, 20,\n",
            "        16, 22, 11, 10, 19, 21, 12, 11, 20, 17, 14, 10, 12, 15, 24, 10, 12, 14,\n",
            "        16, 17,  7, 21, 19, 10, 12, 14, 15, 14, 11, 20, 12,  9, 15,  9, 15, 15,\n",
            "        19,  9, 13, 12, 21, 18, 18, 13, 14, 12])\n"
          ]
        }
      ]
    }
  ]
}