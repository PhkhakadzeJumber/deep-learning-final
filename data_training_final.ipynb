{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156b1df7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-24T10:56:58.405459Z",
     "iopub.status.busy": "2026-01-24T10:56:58.404925Z",
     "iopub.status.idle": "2026-01-24T10:57:09.423172Z",
     "shell.execute_reply": "2026-01-24T10:57:09.422538Z"
    },
    "papermill": {
     "duration": 11.024368,
     "end_time": "2026-01-24T10:57:09.424911",
     "exception": false,
     "start_time": "2026-01-24T10:56:58.400543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. SETUP & IMPORTS\n",
    "# ==========================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "import torchvision.models as models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import shutil\n",
    "import pickle\n",
    "import json\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a241e9d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T10:57:09.430980Z",
     "iopub.status.busy": "2026-01-24T10:57:09.430433Z",
     "iopub.status.idle": "2026-01-24T10:57:09.490743Z",
     "shell.execute_reply": "2026-01-24T10:57:09.490158Z"
    },
    "papermill": {
     "duration": 0.064988,
     "end_time": "2026-01-24T10:57:09.492424",
     "exception": false,
     "start_time": "2026-01-24T10:57:09.427436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# BEST PRACTICE: Set seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ed5275c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T10:57:09.498556Z",
     "iopub.status.busy": "2026-01-24T10:57:09.497980Z",
     "iopub.status.idle": "2026-01-24T10:57:36.270736Z",
     "shell.execute_reply": "2026-01-24T10:57:36.269793Z"
    },
    "papermill": {
     "duration": 26.777524,
     "end_time": "2026-01-24T10:57:36.272371",
     "exception": false,
     "start_time": "2026-01-24T10:57:09.494847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First (image, caption) pair:\n",
      "('1000268201_693b08cb0e.jpg', 'a child in a pink dress is climbing up a set of stairs in an entry way .')\n",
      "\n",
      "Total word tokens: 437,601\n",
      "Unique words: 8,488\n",
      "Final vocabulary size (with special tokens): 8,492\n",
      "\n",
      "Loaded GloVe vectors: 400,000\n",
      "\n",
      "Words in vocab (incl. specials): 8,492\n",
      "Words checked (no specials): 8,488\n",
      "Words found in GloVe: 7,828\n",
      "Words missing from GloVe: 660\n",
      "GloVe coverage: 92.22%\n",
      "\n",
      "Examples of missing words:\n",
      "['fingerpaints', 'aross', 'belays', 'frolicks', 'moutains', 'magizine', 'overshirt', 'anouther', 'jumphouse', 'rappels', 'rappeling', 'barrior', 'torwards', 'bloe', 'inground', 'litlle', 'colred', 'carying', 'wakeboarder', 'waterskier']\n"
     ]
    }
   ],
   "source": [
    "file_path = '/kaggle/input/flickr8k/captions.txt'\n",
    "\n",
    "img_caption_pairs = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "lines = lines[1:]  # skipped header\n",
    "\n",
    "for line in lines:\n",
    "    img, caption = line.split(',', 1)\n",
    "    img_caption_pairs.append((img, caption.lower()))\n",
    "\n",
    "print(\"First (image, caption) pair:\")\n",
    "print(img_caption_pairs[0])\n",
    "\n",
    "# 2. tokenizer (word-level) for Glove embeddings\n",
    "def tokenize(caption):\n",
    "    return re.findall(r\"[a-z0-9]+\", caption.lower())\n",
    "\n",
    "# Building world-level vocabulary from captions we have\n",
    "all_words = []\n",
    "for _, caption in img_caption_pairs:\n",
    "    all_words.extend(tokenize(caption))\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "print(f\"\\nTotal word tokens: {len(all_words):,}\")\n",
    "print(f\"Unique words: {len(word_counts):,}\")\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = ['<pad>', '<unk>', '<s>', '</s>']\n",
    "vocab_words = special_tokens + list(word_counts.keys())\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(vocab_words)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "print(f\"Final vocabulary size (with special tokens): {len(word2idx):,}\")\n",
    "\n",
    "# Loading GloVe 300d pretrained embeddings\n",
    "glove_path = '/kaggle/input/glove-embeddings/glove.6B.300d.txt'\n",
    "\n",
    "glove_dict = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=np.float32)\n",
    "        glove_dict[word] = vector\n",
    "\n",
    "print(f\"\\nLoaded GloVe vectors: {len(glove_dict):,}\")\n",
    "\n",
    "# Computing GloVe coverage of our words we have in vocab\n",
    "real_vocab = [w for w in vocab_words if w not in special_tokens]\n",
    "\n",
    "covered_words = []\n",
    "missing_words = []\n",
    "\n",
    "for w in real_vocab:\n",
    "    if w in glove_dict:\n",
    "        covered_words.append(w)\n",
    "    else: missing_words.append(w)\n",
    "\n",
    "coverage_pct = len(covered_words) / len(real_vocab) * 100\n",
    "\n",
    "print(f\"\\nWords in vocab (incl. specials): {len(vocab_words):,}\")\n",
    "print(f\"Words checked (no specials): {len(real_vocab):,}\")\n",
    "print(f\"Words found in GloVe: {len(covered_words):,}\")\n",
    "print(f\"Words missing from GloVe: {len(missing_words):,}\")\n",
    "print(f\"GloVe coverage: {coverage_pct:.2f}%\")\n",
    "\n",
    "print(\"\\nExamples of missing words:\")\n",
    "print(missing_words[:20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3874c6d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T10:57:36.278407Z",
     "iopub.status.busy": "2026-01-24T10:57:36.278143Z",
     "iopub.status.idle": "2026-01-24T10:57:36.381291Z",
     "shell.execute_reply": "2026-01-24T10:57:36.380482Z"
    },
    "papermill": {
     "duration": 0.107848,
     "end_time": "2026-01-24T10:57:36.382764",
     "exception": false,
     "start_time": "2026-01-24T10:57:36.274916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Mapped 7828/8488 words (92.2% coverage)\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(word2idx, glove_dict, embed_dim=300):\n",
    "    vocab_size = len(word2idx)\n",
    "    embedding_matrix = np.random.randn(vocab_size, embed_dim).astype('float32') * 0.02\n",
    "\n",
    "    found = 0\n",
    "    for word, idx in word2idx.items():\n",
    "        if word in glove_dict:\n",
    "            embedding_matrix[idx] = glove_dict[word]\n",
    "            found += 1\n",
    "\n",
    "    # pad token â†’ zero vector\n",
    "    embedding_matrix[word2idx['<pad>']] = np.zeros(embed_dim)\n",
    "\n",
    "    real_tokens = [w for w in word2idx if w not in ['<pad>', '<unk>', '<s>', '</s>']]\n",
    "    found = sum(1 for w in real_tokens if w in glove_dict)\n",
    "\n",
    "    print(f\"âœ… Mapped {found}/{len(real_tokens)} words \"\n",
    "          f\"({100*found/len(real_tokens):.1f}% coverage)\")\n",
    "\n",
    "    return torch.tensor(embedding_matrix)\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(word2idx, glove_dict, embed_dim=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb455f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T10:57:36.389448Z",
     "iopub.status.busy": "2026-01-24T10:57:36.388978Z",
     "iopub.status.idle": "2026-01-24T10:57:36.414998Z",
     "shell.execute_reply": "2026-01-24T10:57:36.414240Z"
    },
    "papermill": {
     "duration": 0.031137,
     "end_time": "2026-01-24T10:57:36.416530",
     "exception": false,
     "start_time": "2026-01-24T10:57:36.385393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save word2idx\n",
    "with open('/kaggle/working/word2idx.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx, f, ensure_ascii=False)\n",
    "\n",
    "# Save idx2word\n",
    "with open('/kaggle/working/idx2word.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word, f, ensure_ascii=False)\n",
    "\n",
    "# Save special tokens indices\n",
    "special_token_ids = {\n",
    "    'pad': word2idx['<pad>'],\n",
    "    'unk': word2idx['<unk>'],\n",
    "    'start': word2idx['<s>'],\n",
    "    'end': word2idx['</s>']\n",
    "}\n",
    "with open('/kaggle/working/special_tokens.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(special_token_ids, f, ensure_ascii=False)\n",
    "\n",
    "# Save GloVe embedding matrix for inference\n",
    "np.save('/kaggle/working/embedding_matrix.npy', embedding_matrix.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b7b8c85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T10:57:36.422551Z",
     "iopub.status.busy": "2026-01-24T10:57:36.422307Z",
     "iopub.status.idle": "2026-01-24T10:57:36.433901Z",
     "shell.execute_reply": "2026-01-24T10:57:36.433184Z"
    },
    "papermill": {
     "duration": 0.016252,
     "end_time": "2026-01-24T10:57:36.435319",
     "exception": false,
     "start_time": "2026-01-24T10:57:36.419067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test caption:\n",
      "  Original: a child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "  Encoded:  [2, 4, 5, 6, 4, 7, 8, 9, 10, 11, 4, 12, 13, 14, 6, 15, 16, 17, 3]\n",
      "  Decoded:  a child in a pink dress is climbing up a set of stairs in an entry way\n"
     ]
    }
   ],
   "source": [
    "def encode_caption(caption, word2idx, max_len=50):\n",
    "    \"\"\"Convert caption to token IDs\"\"\"\n",
    "    tokens = tokenize(caption)\n",
    "    token_ids = [word2idx['<s>']]  # Start token\n",
    "    \n",
    "    for token in tokens:\n",
    "        token_ids.append(word2idx.get(token, word2idx['<unk>']))\n",
    "    \n",
    "    token_ids.append(word2idx['</s>'])  # End token\n",
    "    \n",
    "    # Truncate if too long\n",
    "    if len(token_ids) > max_len:\n",
    "        token_ids = token_ids[:max_len-1] + [word2idx['</s>']]\n",
    "    \n",
    "    return token_ids\n",
    "\n",
    "def decode_caption(token_ids, idx2word):\n",
    "    \"\"\"Convert token IDs back to text\"\"\"\n",
    "    words = []\n",
    "    for idx in token_ids:\n",
    "        word = idx2word.get(idx, '<unk>')\n",
    "        if word in ['<pad>', '<s>', '</s>']:\n",
    "            continue\n",
    "        words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Test  tokenization (just in case)\n",
    "test_caption = img_caption_pairs[0][1]\n",
    "print(\"Test caption:\")\n",
    "print(f\"  Original: {test_caption}\")\n",
    "\n",
    "encoded = encode_caption(test_caption, word2idx)\n",
    "print(f\"  Encoded:  {encoded}\")\n",
    "\n",
    "decoded = decode_caption(encoded, idx2word)\n",
    "print(f\"  Decoded:  {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b865e45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T10:57:36.442076Z",
     "iopub.status.busy": "2026-01-24T10:57:36.441571Z",
     "iopub.status.idle": "2026-01-24T10:57:36.446745Z",
     "shell.execute_reply": "2026-01-24T10:57:36.446084Z"
    },
    "papermill": {
     "duration": 0.009984,
     "end_time": "2026-01-24T10:57:36.448071",
     "exception": false,
     "start_time": "2026-01-24T10:57:36.438087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, img_caption_pairs, word2idx, image_root, transform=None):\n",
    "        self.data = img_caption_pairs\n",
    "        self.word2idx = word2idx\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, caption = self.data[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_path = f\"{self.image_root}/{img_name}\"\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Tokenize caption\n",
    "        caption_ids = encode_caption(caption, self.word2idx, max_len=50)\n",
    "        caption_tensor = torch.tensor(caption_ids, dtype=torch.long)\n",
    "\n",
    "        return image, caption_tensor, len(caption_tensor)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccf6e252",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T10:57:36.454475Z",
     "iopub.status.busy": "2026-01-24T10:57:36.454006Z",
     "iopub.status.idle": "2026-01-24T10:57:36.457944Z",
     "shell.execute_reply": "2026-01-24T10:57:36.457264Z"
    },
    "papermill": {
     "duration": 0.008526,
     "end_time": "2026-01-24T10:57:36.459295",
     "exception": false,
     "start_time": "2026-01-24T10:57:36.450769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, captions, lengths = zip(*batch)\n",
    "\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    captions_padded = pad_sequence(\n",
    "        captions,\n",
    "        batch_first=True,\n",
    "        padding_value=word2idx['<pad>']\n",
    "    )\n",
    "\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    return images, captions_padded, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ec06cb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T10:57:36.465602Z",
     "iopub.status.busy": "2026-01-24T10:57:36.465117Z",
     "iopub.status.idle": "2026-01-24T10:57:36.497858Z",
     "shell.execute_reply": "2026-01-24T10:57:36.497210Z"
    },
    "papermill": {
     "duration": 0.037347,
     "end_time": "2026-01-24T10:57:36.499180",
     "exception": false,
     "start_time": "2026-01-24T10:57:36.461833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train/val split\n",
    "\n",
    "img_to_captions = defaultdict(list)\n",
    "\n",
    "for img, caption in img_caption_pairs:\n",
    "  img_to_captions[img].append(caption)\n",
    "\n",
    "all_images = list(img_to_captions.keys())\n",
    "\n",
    "train_images, val_images = train_test_split(\n",
    "    all_images,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "train_pairs = []\n",
    "val_pairs = []\n",
    "\n",
    "for img in train_images:\n",
    "  for caption in img_to_captions[img]:\n",
    "    train_pairs.append((img, caption))\n",
    "\n",
    "for img in val_images:\n",
    "  for caption in img_to_captions[img]:\n",
    "    val_pairs.append((img, caption))\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])  # ImageNet stats for pretrained CNN\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5af375c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T10:57:36.505737Z",
     "iopub.status.busy": "2026-01-24T10:57:36.505234Z",
     "iopub.status.idle": "2026-01-24T10:57:38.107084Z",
     "shell.execute_reply": "2026-01-24T10:57:38.103426Z"
    },
    "papermill": {
     "duration": 1.608272,
     "end_time": "2026-01-24T10:57:38.110093",
     "exception": false,
     "start_time": "2026-01-24T10:57:36.501821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: torch.Size([64, 3, 256, 256])\n",
      "Captions: torch.Size([64, 21])\n",
      "Lengths: tensor([16,  8, 12,  9, 16, 16, 13,  9, 11, 15,  8, 11, 11, 11, 19, 15, 10, 18,\n",
      "        15, 21, 10,  9, 16, 19, 11, 10, 16, 14, 13,  9, 11, 14, 21, 10, 11, 14,\n",
      "        15, 16,  7, 18, 18,  9, 11, 13, 14, 13, 10, 18, 11,  8, 14,  9, 14, 14,\n",
      "        15,  8, 12, 11, 20, 17, 17, 12, 13, 11])\n"
     ]
    }
   ],
   "source": [
    "image_root = '/kaggle/input/flickr8k/Images'\n",
    "\n",
    "train_dataset = ImageCaptionDataset(train_pairs, word2idx, image_root, transform=image_transform)\n",
    "val_dataset = ImageCaptionDataset(val_pairs, word2idx, image_root, transform=image_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    collate_fn=collate_fn,\n",
    "    persistent_workers=True   # keeps workers alive between epochs\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "images, captions, lengths = next(iter(train_loader))\n",
    "\n",
    "print(\"Images:\", images.shape)        # (B, 3, 256, 256)\n",
    "print(\"Captions:\", captions.shape)    # (B, max_len)\n",
    "print(\"Lengths:\", lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a62a169e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T10:57:38.131719Z",
     "iopub.status.busy": "2026-01-24T10:57:38.128614Z",
     "iopub.status.idle": "2026-01-24T10:57:39.977829Z",
     "shell.execute_reply": "2026-01-24T10:57:39.976351Z"
    },
    "papermill": {
     "duration": 1.865431,
     "end_time": "2026-01-24T10:57:39.983063",
     "exception": false,
     "start_time": "2026-01-24T10:57:38.117632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 144MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 44,287,740\n",
      "Trainable Parameters: 18,232,108\n",
      "Frozen Parameters: 26,055,632\n"
     ]
    }
   ],
   "source": [
    "class ImgToCaptionModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embed_dim=300, hidden_dim=512, max_seq_len=50, pad_token_id=0):\n",
    "        super().__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.pad_token_id = pad_token_id\n",
    "        \n",
    "        # IMAGE ENCODER (Pretrained CNN)\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        # FREEZE pretrained CNN\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # IMAGE PROJECTION\n",
    "        self.img_proj = nn.Sequential(\n",
    "            nn.Linear(2048, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # 2D positional embeddings for 8x8 feature map\n",
    "        self.img_pos_embed = nn.Parameter(torch.randn(1, 64, hidden_dim) * 0.02)\n",
    "        \n",
    "        # TEXT DECODER (GloVe Embeddings)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight = nn.Parameter(embedding_matrix.clone())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # TEXT PROJECTION\n",
    "        self.word_proj = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Learnable positional embeddings\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(max_seq_len, hidden_dim) * 0.02)\n",
    "        \n",
    "        # TRANSFORMER DECODER\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=3\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        # IMAGE ENCODING (Frozen CNN)\n",
    "        with torch.no_grad():\n",
    "            img_features = self.cnn(images)  # (B, 2048, 8, 8)\n",
    "        \n",
    "        img_features = img_features.flatten(2).permute(0, 2, 1)  # (B, 64, 2048)\n",
    "        img_features = self.img_proj(img_features)  # (B, 64, hidden_dim)\n",
    "        img_features = img_features + self.img_pos_embed \n",
    "        \n",
    "        # TEXT ENCODING (Frozen GloVe)\n",
    "        seq_len = captions.size(1)\n",
    "        caption_embeds = self.embedding(captions)  # (B, seq_len, 300)\n",
    "        caption_embeds = self.word_proj(caption_embeds)  # (B, seq_len, hidden_dim)\n",
    "        caption_embeds = caption_embeds + self.pos_encoding[:seq_len].unsqueeze(0)\n",
    "        \n",
    "        # MASKS\n",
    "        tgt_mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=captions.device, dtype=torch.bool),\n",
    "            diagonal=1\n",
    "        )\n",
    "        \n",
    "        tgt_key_padding_mask = (captions == self.pad_token_id)\n",
    "        \n",
    "        # DECODER\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=caption_embeds,\n",
    "            memory=img_features,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        logits = self.fc_out(output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# initialize model\n",
    "model = ImgToCaptionModel(\n",
    "    embedding_matrix=embedding_matrix,  # GloVe matrix\n",
    "    embed_dim=300,                      # GloVe dimension\n",
    "    hidden_dim=512,                     # Transformer hidden size\n",
    "    max_seq_len=50,\n",
    "    pad_token_id=word2idx['<pad>']\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# counting total params\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen Parameters: {frozen_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13555299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T10:57:40.002369Z",
     "iopub.status.busy": "2026-01-24T10:57:40.002005Z",
     "iopub.status.idle": "2026-01-24T10:57:40.006505Z",
     "shell.execute_reply": "2026-01-24T10:57:40.005719Z"
    },
    "papermill": {
     "duration": 0.014301,
     "end_time": "2026-01-24T10:57:40.009285",
     "exception": false,
     "start_time": "2026-01-24T10:57:39.994984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perplexity metric calculation (clearer indicator than just raw loss function)\n",
    "def perplexity_from_loss(loss):\n",
    "    return math.exp(min(loss, 100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dbb73fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T10:57:40.050977Z",
     "iopub.status.busy": "2026-01-24T10:57:40.050179Z",
     "iopub.status.idle": "2026-01-24T11:30:02.587744Z",
     "shell.execute_reply": "2026-01-24T11:30:02.586680Z"
    },
    "papermill": {
     "duration": 1942.560335,
     "end_time": "2026-01-24T11:30:02.592287",
     "exception": false,
     "start_time": "2026-01-24T10:57:40.031952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Best model saved!\n",
      "\n",
      "ðŸ“Š Epoch [1/15]\n",
      "   Train Loss: 4.4627\n",
      "   Train PPL:  86.72\n",
      "   Val Loss:   3.8631\n",
      "   Val PPL:    47.61\n",
      "   LR:         0.000100\n",
      "âœ… Best model saved!\n",
      "\n",
      "ðŸ“Š Epoch [2/15]\n",
      "   Train Loss: 3.6586\n",
      "   Train PPL:  38.81\n",
      "   Val Loss:   3.5805\n",
      "   Val PPL:    35.89\n",
      "   LR:         0.000100\n",
      "âœ… Best model saved!\n",
      "\n",
      "ðŸ“Š Epoch [3/15]\n",
      "   Train Loss: 3.3920\n",
      "   Train PPL:  29.72\n",
      "   Val Loss:   3.4538\n",
      "   Val PPL:    31.62\n",
      "   LR:         0.000100\n",
      "âœ… Best model saved!\n",
      "\n",
      "ðŸ“Š Epoch [4/15]\n",
      "   Train Loss: 3.2099\n",
      "   Train PPL:  24.78\n",
      "   Val Loss:   3.3797\n",
      "   Val PPL:    29.36\n",
      "   LR:         0.000100\n",
      "âœ… Best model saved!\n",
      "\n",
      "ðŸ“Š Epoch [5/15]\n",
      "   Train Loss: 3.0645\n",
      "   Train PPL:  21.42\n",
      "   Val Loss:   3.3256\n",
      "   Val PPL:    27.82\n",
      "   LR:         0.000100\n",
      "âœ… Best model saved!\n",
      "\n",
      "ðŸ“Š Epoch [6/15]\n",
      "   Train Loss: 2.9402\n",
      "   Train PPL:  18.92\n",
      "   Val Loss:   3.3028\n",
      "   Val PPL:    27.19\n",
      "   LR:         0.000100\n",
      "âœ… Best model saved!\n",
      "\n",
      "ðŸ“Š Epoch [7/15]\n",
      "   Train Loss: 2.8325\n",
      "   Train PPL:  16.99\n",
      "   Val Loss:   3.2986\n",
      "   Val PPL:    27.07\n",
      "   LR:         0.000100\n",
      "\n",
      "ðŸ“Š Epoch [8/15]\n",
      "   Train Loss: 2.7306\n",
      "   Train PPL:  15.34\n",
      "   Val Loss:   3.2996\n",
      "   Val PPL:    27.10\n",
      "   LR:         0.000100\n",
      "âœ… Best model saved!\n",
      "\n",
      "ðŸ“Š Epoch [9/15]\n",
      "   Train Loss: 2.6415\n",
      "   Train PPL:  14.03\n",
      "   Val Loss:   3.2944\n",
      "   Val PPL:    26.96\n",
      "   LR:         0.000100\n",
      "\n",
      "ðŸ“Š Epoch [10/15]\n",
      "   Train Loss: 2.5537\n",
      "   Train PPL:  12.85\n",
      "   Val Loss:   3.3016\n",
      "   Val PPL:    27.16\n",
      "   LR:         0.000100\n",
      "\n",
      "ðŸ“Š Epoch [11/15]\n",
      "   Train Loss: 2.4724\n",
      "   Train PPL:  11.85\n",
      "   Val Loss:   3.3120\n",
      "   Val PPL:    27.44\n",
      "   LR:         0.000100\n",
      "\n",
      "ðŸ“Š Epoch [12/15]\n",
      "   Train Loss: 2.3965\n",
      "   Train PPL:  10.98\n",
      "   Val Loss:   3.3259\n",
      "   Val PPL:    27.82\n",
      "   LR:         0.000100\n",
      "\n",
      "ðŸ“Š Epoch [13/15]\n",
      "   Train Loss: 2.3223\n",
      "   Train PPL:  10.20\n",
      "   Val Loss:   3.3396\n",
      "   Val PPL:    28.21\n",
      "   LR:         0.000050\n",
      "\n",
      "ðŸ“Š Epoch [14/15]\n",
      "   Train Loss: 2.1959\n",
      "   Train PPL:  8.99\n",
      "   Val Loss:   3.3462\n",
      "   Val PPL:    28.39\n",
      "   LR:         0.000050\n",
      "\n",
      "ðŸ“Š Epoch [15/15]\n",
      "   Train Loss: 2.1455\n",
      "   Train PPL:  8.55\n",
      "   Val Loss:   3.3739\n",
      "   Val PPL:    29.19\n",
      "   LR:         0.000050\n",
      "\n",
      "âœ… Training complete!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15                   \n",
    "learning_rate = 1e-4  \n",
    "\n",
    "pad_token_id = word2idx['<pad>']\n",
    "start_token_id = word2idx['<s>']\n",
    "end_token_id = word2idx['</s>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id, label_smoothing=0.05)\n",
    "\n",
    "# Filtering out frozen parameters\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.AdamW(trainable_params, lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler \n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# For saving best model\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # ========== TRAIN ==========\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, captions, lengths in train_loader:\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        inputs = captions[:, :-1]\n",
    "        targets = captions[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(images, inputs)\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, outputs.size(-1)),\n",
    "                targets.reshape(-1)\n",
    "            )\n",
    "\n",
    "        # Backprop with AMP\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscale before clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_ppl = perplexity_from_loss(avg_train_loss)\n",
    "\n",
    "    # ========== VALIDATION ==========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    bleu_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in val_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            inputs = captions[:, :-1]\n",
    "            targets = captions[:, 1:]\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(images, inputs)\n",
    "                loss = criterion(\n",
    "                    outputs.reshape(-1, outputs.size(-1)),\n",
    "                    targets.reshape(-1)\n",
    "                )\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_ppl = perplexity_from_loss(avg_val_loss)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), '/kaggle/working/image_caption_model_best.pth')\n",
    "        print(\"âœ… Best model saved!\")\n",
    "\n",
    "    print(f\"\\nðŸ“Š Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"   Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"   Train PPL:  {train_ppl:.2f}\")\n",
    "    print(f\"   Val Loss:   {avg_val_loss:.4f}\")\n",
    "    print(f\"   Val PPL:    {val_ppl:.2f}\")\n",
    "    print(f\"   LR:         {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed1115bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T11:30:02.602444Z",
     "iopub.status.busy": "2026-01-24T11:30:02.601545Z",
     "iopub.status.idle": "2026-01-24T11:30:02.812252Z",
     "shell.execute_reply": "2026-01-24T11:30:02.811625Z"
    },
    "papermill": {
     "duration": 0.217689,
     "end_time": "2026-01-24T11:30:02.813904",
     "exception": false,
     "start_time": "2026-01-24T11:30:02.596215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model after 15 epochs\n",
    "torch.save(model.state_dict(), '/kaggle/working/image_caption_model_after.pth')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 623289,
     "sourceId": 1111676,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 870709,
     "sourceId": 1483651,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1989.775036,
   "end_time": "2026-01-24T11:30:05.637322",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-24T10:56:55.862286",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
