{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f0ede7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-24T09:41:32.806813Z",
     "iopub.status.busy": "2026-01-24T09:41:32.806492Z",
     "iopub.status.idle": "2026-01-24T09:41:48.891315Z",
     "shell.execute_reply": "2026-01-24T09:41:48.890480Z"
    },
    "papermill": {
     "duration": 16.090952,
     "end_time": "2026-01-24T09:41:48.893237",
     "exception": false,
     "start_time": "2026-01-24T09:41:32.802285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. SETUP & IMPORTS\n",
    "# ==========================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "import torchvision.models as models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efb51bf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T09:41:48.899281Z",
     "iopub.status.busy": "2026-01-24T09:41:48.898881Z",
     "iopub.status.idle": "2026-01-24T09:41:48.958242Z",
     "shell.execute_reply": "2026-01-24T09:41:48.957347Z"
    },
    "papermill": {
     "duration": 0.064038,
     "end_time": "2026-01-24T09:41:48.959804",
     "exception": false,
     "start_time": "2026-01-24T09:41:48.895766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# BEST PRACTICE: Set seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f4a5e60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T09:41:48.965933Z",
     "iopub.status.busy": "2026-01-24T09:41:48.965403Z",
     "iopub.status.idle": "2026-01-24T09:42:17.340128Z",
     "shell.execute_reply": "2026-01-24T09:42:17.339097Z"
    },
    "papermill": {
     "duration": 28.379698,
     "end_time": "2026-01-24T09:42:17.341913",
     "exception": false,
     "start_time": "2026-01-24T09:41:48.962215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First (image, caption) pair:\n",
      "('1000268201_693b08cb0e.jpg', 'a child in a pink dress is climbing up a set of stairs in an entry way .')\n",
      "\n",
      "Total word tokens: 437,601\n",
      "Unique words: 8,488\n",
      "Final vocabulary size (with special tokens): 8,492\n",
      "\n",
      "Loaded GloVe vectors: 400,000\n",
      "\n",
      "Words in vocab (incl. specials): 8,492\n",
      "Words checked (no specials): 8,488\n",
      "Words found in GloVe: 7,828\n",
      "Words missing from GloVe: 660\n",
      "GloVe coverage: 92.22%\n",
      "\n",
      "Examples of missing words:\n",
      "['fingerpaints', 'aross', 'belays', 'frolicks', 'moutains', 'magizine', 'overshirt', 'anouther', 'jumphouse', 'rappels', 'rappeling', 'barrior', 'torwards', 'bloe', 'inground', 'litlle', 'colred', 'carying', 'wakeboarder', 'waterskier']\n"
     ]
    }
   ],
   "source": [
    "file_path = '/kaggle/input/flickr8k/captions.txt'\n",
    "\n",
    "img_caption_pairs = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "lines = lines[1:]  # skipped header\n",
    "\n",
    "for line in lines:\n",
    "    img, caption = line.split(',', 1)\n",
    "    img_caption_pairs.append((img, caption.lower()))\n",
    "\n",
    "print(\"First (image, caption) pair:\")\n",
    "print(img_caption_pairs[0])\n",
    "\n",
    "# 2. tokenizer (word-level) for Glove embeddings\n",
    "def tokenize(caption):\n",
    "    return re.findall(r\"[a-z0-9]+\", caption.lower())\n",
    "\n",
    "# Building world-level vocabulary from captions we have\n",
    "all_words = []\n",
    "for _, caption in img_caption_pairs:\n",
    "    all_words.extend(tokenize(caption))\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "print(f\"\\nTotal word tokens: {len(all_words):,}\")\n",
    "print(f\"Unique words: {len(word_counts):,}\")\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = ['<pad>', '<unk>', '<s>', '</s>']\n",
    "vocab_words = special_tokens + list(word_counts.keys())\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(vocab_words)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "print(f\"Final vocabulary size (with special tokens): {len(word2idx):,}\")\n",
    "\n",
    "# Loading GloVe 300d pretrained embeddings\n",
    "glove_path = '/kaggle/input/glove-embeddings/glove.6B.300d.txt'\n",
    "\n",
    "glove_dict = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=np.float32)\n",
    "        glove_dict[word] = vector\n",
    "\n",
    "print(f\"\\nLoaded GloVe vectors: {len(glove_dict):,}\")\n",
    "\n",
    "# Computing GloVe coverage of our words we have in vocab\n",
    "real_vocab = [w for w in vocab_words if w not in special_tokens]\n",
    "\n",
    "covered_words = []\n",
    "missing_words = []\n",
    "\n",
    "for w in real_vocab:\n",
    "    if w in glove_dict:\n",
    "        covered_words.append(w)\n",
    "    else: missing_words.append(w)\n",
    "\n",
    "coverage_pct = len(covered_words) / len(real_vocab) * 100\n",
    "\n",
    "print(f\"\\nWords in vocab (incl. specials): {len(vocab_words):,}\")\n",
    "print(f\"Words checked (no specials): {len(real_vocab):,}\")\n",
    "print(f\"Words found in GloVe: {len(covered_words):,}\")\n",
    "print(f\"Words missing from GloVe: {len(missing_words):,}\")\n",
    "print(f\"GloVe coverage: {coverage_pct:.2f}%\")\n",
    "\n",
    "print(\"\\nExamples of missing words:\")\n",
    "print(missing_words[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ba5b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T09:42:17.348929Z",
     "iopub.status.busy": "2026-01-24T09:42:17.348596Z",
     "iopub.status.idle": "2026-01-24T09:42:17.463841Z",
     "shell.execute_reply": "2026-01-24T09:42:17.463017Z"
    },
    "papermill": {
     "duration": 0.120535,
     "end_time": "2026-01-24T09:42:17.465426",
     "exception": false,
     "start_time": "2026-01-24T09:42:17.344891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Mapped 7828/8488 words (92.2% coverage)\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(word2idx, glove_dict, embed_dim=300):\n",
    "    vocab_size = len(word2idx)\n",
    "    embedding_matrix = np.random.randn(vocab_size, embed_dim).astype('float32') * 0.02\n",
    "\n",
    "    found = 0\n",
    "    for word, idx in word2idx.items():\n",
    "        if word in glove_dict:\n",
    "            embedding_matrix[idx] = glove_dict[word]\n",
    "            found += 1\n",
    "\n",
    "    # pad token â†’ zero vector\n",
    "    embedding_matrix[word2idx['<pad>']] = np.zeros(embed_dim)\n",
    "\n",
    "    real_tokens = [w for w in word2idx if w not in ['<pad>', '<unk>', '<s>', '</s>']]\n",
    "    found = sum(1 for w in real_tokens if w in glove_dict)\n",
    "\n",
    "    print(f\"âœ… Mapped {found}/{len(real_tokens)} words \"\n",
    "          f\"({100*found/len(real_tokens):.1f}% coverage)\")\n",
    "\n",
    "    return torch.tensor(embedding_matrix)\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(word2idx, glove_dict, embed_dim=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "964412ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T09:42:17.471679Z",
     "iopub.status.busy": "2026-01-24T09:42:17.471399Z",
     "iopub.status.idle": "2026-01-24T09:42:17.483912Z",
     "shell.execute_reply": "2026-01-24T09:42:17.483045Z"
    },
    "papermill": {
     "duration": 0.017326,
     "end_time": "2026-01-24T09:42:17.485296",
     "exception": false,
     "start_time": "2026-01-24T09:42:17.467970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test caption:\n",
      "  Original: a child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "  Encoded:  [2, 4, 5, 6, 4, 7, 8, 9, 10, 11, 4, 12, 13, 14, 6, 15, 16, 17, 3]\n",
      "  Decoded:  a child in a pink dress is climbing up a set of stairs in an entry way\n"
     ]
    }
   ],
   "source": [
    "def encode_caption(caption, word2idx, max_len=50):\n",
    "    \"\"\"Convert caption to token IDs\"\"\"\n",
    "    tokens = tokenize(caption)\n",
    "    token_ids = [word2idx['<s>']]  # Start token\n",
    "    \n",
    "    for token in tokens:\n",
    "        token_ids.append(word2idx.get(token, word2idx['<unk>']))\n",
    "    \n",
    "    token_ids.append(word2idx['</s>'])  # End token\n",
    "    \n",
    "    # Truncate if too long\n",
    "    if len(token_ids) > max_len:\n",
    "        token_ids = token_ids[:max_len-1] + [word2idx['</s>']]\n",
    "    \n",
    "    return token_ids\n",
    "\n",
    "def decode_caption(token_ids, idx2word):\n",
    "    \"\"\"Convert token IDs back to text\"\"\"\n",
    "    words = []\n",
    "    for idx in token_ids:\n",
    "        word = idx2word.get(idx, '<unk>')\n",
    "        if word in ['<pad>', '<s>', '</s>']:\n",
    "            continue\n",
    "        words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Test  tokenization (just in case)\n",
    "test_caption = img_caption_pairs[0][1]\n",
    "print(\"Test caption:\")\n",
    "print(f\"  Original: {test_caption}\")\n",
    "\n",
    "encoded = encode_caption(test_caption, word2idx)\n",
    "print(f\"  Encoded:  {encoded}\")\n",
    "\n",
    "decoded = decode_caption(encoded, idx2word)\n",
    "print(f\"  Decoded:  {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f036fe6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T09:42:17.491313Z",
     "iopub.status.busy": "2026-01-24T09:42:17.491066Z",
     "iopub.status.idle": "2026-01-24T09:42:17.496200Z",
     "shell.execute_reply": "2026-01-24T09:42:17.495583Z"
    },
    "papermill": {
     "duration": 0.009783,
     "end_time": "2026-01-24T09:42:17.497501",
     "exception": false,
     "start_time": "2026-01-24T09:42:17.487718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, img_caption_pairs, word2idx, image_root, transform=None):\n",
    "        self.data = img_caption_pairs\n",
    "        self.word2idx = word2idx\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, caption = self.data[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_path = f\"{self.image_root}/{img_name}\"\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Tokenize caption\n",
    "        caption_ids = encode_caption(caption, self.word2idx, max_len=50)\n",
    "        caption_tensor = torch.tensor(caption_ids, dtype=torch.long)\n",
    "\n",
    "        return image, caption_tensor, len(caption_tensor)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb63ad73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T09:42:17.503527Z",
     "iopub.status.busy": "2026-01-24T09:42:17.503037Z",
     "iopub.status.idle": "2026-01-24T09:42:17.507102Z",
     "shell.execute_reply": "2026-01-24T09:42:17.506515Z"
    },
    "papermill": {
     "duration": 0.008517,
     "end_time": "2026-01-24T09:42:17.508421",
     "exception": false,
     "start_time": "2026-01-24T09:42:17.499904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, captions, lengths = zip(*batch)\n",
    "\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    captions_padded = pad_sequence(\n",
    "        captions,\n",
    "        batch_first=True,\n",
    "        padding_value=word2idx['<pad>']\n",
    "    )\n",
    "\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    return images, captions_padded, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe9c5650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T09:42:17.514514Z",
     "iopub.status.busy": "2026-01-24T09:42:17.514289Z",
     "iopub.status.idle": "2026-01-24T09:42:17.547759Z",
     "shell.execute_reply": "2026-01-24T09:42:17.547158Z"
    },
    "papermill": {
     "duration": 0.038227,
     "end_time": "2026-01-24T09:42:17.549219",
     "exception": false,
     "start_time": "2026-01-24T09:42:17.510992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train/val split\n",
    "\n",
    "img_to_captions = defaultdict(list)\n",
    "\n",
    "for img, caption in img_caption_pairs:\n",
    "  img_to_captions[img].append(caption)\n",
    "\n",
    "all_images = list(img_to_captions.keys())\n",
    "\n",
    "train_images, val_images = train_test_split(\n",
    "    all_images,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "train_pairs = []\n",
    "val_pairs = []\n",
    "\n",
    "for img in train_images:\n",
    "  for caption in img_to_captions[img]:\n",
    "    train_pairs.append((img, caption))\n",
    "\n",
    "for img in val_images:\n",
    "  for caption in img_to_captions[img]:\n",
    "    val_pairs.append((img, caption))\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])  # ImageNet stats for pretrained CNN\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc71cd28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T09:42:17.555286Z",
     "iopub.status.busy": "2026-01-24T09:42:17.555055Z",
     "iopub.status.idle": "2026-01-24T09:42:19.083305Z",
     "shell.execute_reply": "2026-01-24T09:42:19.081446Z"
    },
    "papermill": {
     "duration": 1.536129,
     "end_time": "2026-01-24T09:42:19.087898",
     "exception": false,
     "start_time": "2026-01-24T09:42:17.551769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: torch.Size([64, 3, 256, 256])\n",
      "Captions: torch.Size([64, 21])\n",
      "Lengths: tensor([16,  8, 12,  9, 16, 16, 13,  9, 11, 15,  8, 11, 11, 11, 19, 15, 10, 18,\n",
      "        15, 21, 10,  9, 16, 19, 11, 10, 16, 14, 13,  9, 11, 14, 21, 10, 11, 14,\n",
      "        15, 16,  7, 18, 18,  9, 11, 13, 14, 13, 10, 18, 11,  8, 14,  9, 14, 14,\n",
      "        15,  8, 12, 11, 20, 17, 17, 12, 13, 11])\n"
     ]
    }
   ],
   "source": [
    "image_root = '/kaggle/input/flickr8k/Images'\n",
    "\n",
    "train_dataset = ImageCaptionDataset(train_pairs, word2idx, image_root, transform=image_transform)\n",
    "val_dataset = ImageCaptionDataset(val_pairs, word2idx, image_root, transform=image_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    collate_fn=collate_fn,\n",
    "    persistent_workers=True   # keeps workers alive between epochs\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "images, captions, lengths = next(iter(train_loader))\n",
    "\n",
    "print(\"Images:\", images.shape)        # (B, 3, 256, 256)\n",
    "print(\"Captions:\", captions.shape)    # (B, max_len)\n",
    "print(\"Lengths:\", lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffddee74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T09:42:19.111521Z",
     "iopub.status.busy": "2026-01-24T09:42:19.111037Z",
     "iopub.status.idle": "2026-01-24T09:42:21.064641Z",
     "shell.execute_reply": "2026-01-24T09:42:21.063504Z"
    },
    "papermill": {
     "duration": 1.970372,
     "end_time": "2026-01-24T09:42:21.068867",
     "exception": false,
     "start_time": "2026-01-24T09:42:19.098495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 126MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 44,287,740\n",
      "Trainable Parameters: 18,232,108\n",
      "Frozen Parameters: 26,055,632\n"
     ]
    }
   ],
   "source": [
    "class ImgToCaptionModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embed_dim=300, hidden_dim=512, max_seq_len=50, pad_token_id=0):\n",
    "        super().__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.pad_token_id = pad_token_id\n",
    "        \n",
    "        # IMAGE ENCODER (Pretrained CNN)\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        # FREEZE pretrained CNN\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # IMAGE PROJECTION\n",
    "        self.img_proj = nn.Sequential(\n",
    "            nn.Linear(2048, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # 2D positional embeddings for 8x8 feature map\n",
    "        self.img_pos_embed = nn.Parameter(torch.randn(1, 64, hidden_dim) * 0.02)\n",
    "        \n",
    "        # TEXT DECODER (GloVe Embeddings)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight = nn.Parameter(embedding_matrix.clone())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # TEXT PROJECTION\n",
    "        self.word_proj = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Learnable positional embeddings\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(max_seq_len, hidden_dim) * 0.02)\n",
    "        \n",
    "        # TRANSFORMER DECODER\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=3\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        # IMAGE ENCODING (Frozen CNN)\n",
    "        with torch.no_grad():\n",
    "            img_features = self.cnn(images)  # (B, 2048, 8, 8)\n",
    "        \n",
    "        img_features = img_features.flatten(2).permute(0, 2, 1)  # (B, 64, 2048)\n",
    "        img_features = self.img_proj(img_features)  # (B, 64, hidden_dim)\n",
    "        img_features = img_features + self.img_pos_embed \n",
    "        \n",
    "        # TEXT ENCODING (Frozen GloVe)\n",
    "        seq_len = captions.size(1)\n",
    "        caption_embeds = self.embedding(captions)  # (B, seq_len, 300)\n",
    "        caption_embeds = self.word_proj(caption_embeds)  # (B, seq_len, hidden_dim)\n",
    "        caption_embeds = caption_embeds + self.pos_encoding[:seq_len].unsqueeze(0)\n",
    "        \n",
    "        # MASKS\n",
    "        tgt_mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=captions.device, dtype=torch.bool),\n",
    "            diagonal=1\n",
    "        )\n",
    "        \n",
    "        tgt_key_padding_mask = (captions == self.pad_token_id)\n",
    "        \n",
    "        # DECODER\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=caption_embeds,\n",
    "            memory=img_features,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        logits = self.fc_out(output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# initialize model\n",
    "model = ImgToCaptionModel(\n",
    "    embedding_matrix=embedding_matrix,  # GloVe matrix\n",
    "    embed_dim=300,                      # GloVe dimension\n",
    "    hidden_dim=512,                     # Transformer hidden size\n",
    "    max_seq_len=50,\n",
    "    pad_token_id=word2idx['<pad>']\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# counting total params\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen Parameters: {frozen_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de339516",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T09:42:21.107064Z",
     "iopub.status.busy": "2026-01-24T09:42:21.106668Z",
     "iopub.status.idle": "2026-01-24T09:42:21.119294Z",
     "shell.execute_reply": "2026-01-24T09:42:21.115738Z"
    },
    "papermill": {
     "duration": 0.036401,
     "end_time": "2026-01-24T09:42:21.121901",
     "exception": false,
     "start_time": "2026-01-24T09:42:21.085500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perplexity metric calculation (clearer indicator than just raw loss function)\n",
    "def perplexity_from_loss(loss):\n",
    "    return math.exp(min(loss, 100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "040a37a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T09:42:21.158147Z",
     "iopub.status.busy": "2026-01-24T09:42:21.155360Z",
     "iopub.status.idle": "2026-01-24T10:14:55.466520Z",
     "shell.execute_reply": "2026-01-24T10:14:55.465490Z"
    },
    "papermill": {
     "duration": 1954.33429,
     "end_time": "2026-01-24T10:14:55.472180",
     "exception": false,
     "start_time": "2026-01-24T09:42:21.137890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Best model saved!\n",
      "\n",
      "ðŸ“Š Epoch [1/15]\n",
      "   Train Loss: 4.4627\n",
      "   Train PPL:  86.72\n",
      "   Val Loss:   3.8631\n",
      "   Val PPL:    47.61\n",
      "   LR:         0.000100\n",
      "âœ… Best model saved!\n",
      "\n",
      "ðŸ“Š Epoch [2/15]\n",
      "   Train Loss: 3.6586\n",
      "   Train PPL:  38.81\n",
      "   Val Loss:   3.5805\n",
      "   Val PPL:    35.89\n",
      "   LR:         0.000100\n",
      "âœ… Best model saved!\n",
      "\n",
      "ðŸ“Š Epoch [3/15]\n",
      "   Train Loss: 3.3920\n",
      "   Train PPL:  29.72\n",
      "   Val Loss:   3.4538\n",
      "   Val PPL:    31.62\n",
      "   LR:         0.000100\n",
      "âœ… Best model saved!\n",
      "\n",
      "ðŸ“Š Epoch [4/15]\n",
      "   Train Loss: 3.2099\n",
      "   Train PPL:  24.78\n",
      "   Val Loss:   3.3797\n",
      "   Val PPL:    29.36\n",
      "   LR:         0.000100\n",
      "âœ… Best model saved!\n",
      "\n",
      "ðŸ“Š Epoch [5/15]\n",
      "   Train Loss: 3.0645\n",
      "   Train PPL:  21.42\n",
      "   Val Loss:   3.3256\n",
      "   Val PPL:    27.82\n",
      "   LR:         0.000100\n",
      "âœ… Best model saved!\n",
      "\n",
      "ðŸ“Š Epoch [6/15]\n",
      "   Train Loss: 2.9402\n",
      "   Train PPL:  18.92\n",
      "   Val Loss:   3.3028\n",
      "   Val PPL:    27.19\n",
      "   LR:         0.000100\n",
      "âœ… Best model saved!\n",
      "\n",
      "ðŸ“Š Epoch [7/15]\n",
      "   Train Loss: 2.8325\n",
      "   Train PPL:  16.99\n",
      "   Val Loss:   3.2986\n",
      "   Val PPL:    27.07\n",
      "   LR:         0.000100\n",
      "\n",
      "ðŸ“Š Epoch [8/15]\n",
      "   Train Loss: 2.7306\n",
      "   Train PPL:  15.34\n",
      "   Val Loss:   3.2996\n",
      "   Val PPL:    27.10\n",
      "   LR:         0.000100\n",
      "âœ… Best model saved!\n",
      "\n",
      "ðŸ“Š Epoch [9/15]\n",
      "   Train Loss: 2.6415\n",
      "   Train PPL:  14.03\n",
      "   Val Loss:   3.2944\n",
      "   Val PPL:    26.96\n",
      "   LR:         0.000100\n",
      "\n",
      "ðŸ“Š Epoch [10/15]\n",
      "   Train Loss: 2.5537\n",
      "   Train PPL:  12.85\n",
      "   Val Loss:   3.3016\n",
      "   Val PPL:    27.16\n",
      "   LR:         0.000100\n",
      "\n",
      "ðŸ“Š Epoch [11/15]\n",
      "   Train Loss: 2.4724\n",
      "   Train PPL:  11.85\n",
      "   Val Loss:   3.3120\n",
      "   Val PPL:    27.44\n",
      "   LR:         0.000100\n",
      "\n",
      "ðŸ“Š Epoch [12/15]\n",
      "   Train Loss: 2.3965\n",
      "   Train PPL:  10.98\n",
      "   Val Loss:   3.3259\n",
      "   Val PPL:    27.82\n",
      "   LR:         0.000100\n",
      "\n",
      "ðŸ“Š Epoch [13/15]\n",
      "   Train Loss: 2.3223\n",
      "   Train PPL:  10.20\n",
      "   Val Loss:   3.3396\n",
      "   Val PPL:    28.21\n",
      "   LR:         0.000050\n",
      "\n",
      "ðŸ“Š Epoch [14/15]\n",
      "   Train Loss: 2.1959\n",
      "   Train PPL:  8.99\n",
      "   Val Loss:   3.3462\n",
      "   Val PPL:    28.39\n",
      "   LR:         0.000050\n",
      "\n",
      "ðŸ“Š Epoch [15/15]\n",
      "   Train Loss: 2.1455\n",
      "   Train PPL:  8.55\n",
      "   Val Loss:   3.3739\n",
      "   Val PPL:    29.19\n",
      "   LR:         0.000050\n",
      "\n",
      "âœ… Training complete!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15                   \n",
    "learning_rate = 1e-4  \n",
    "\n",
    "pad_token_id = word2idx['<pad>']\n",
    "start_token_id = word2idx['<s>']\n",
    "end_token_id = word2idx['</s>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id, label_smoothing=0.05)\n",
    "\n",
    "# Filtering out frozen parameters\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.AdamW(trainable_params, lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler \n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# For saving best model\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # ========== TRAIN ==========\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, captions, lengths in train_loader:\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        inputs = captions[:, :-1]\n",
    "        targets = captions[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(images, inputs)\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, outputs.size(-1)),\n",
    "                targets.reshape(-1)\n",
    "            )\n",
    "\n",
    "        # Backprop with AMP\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscale before clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_ppl = perplexity_from_loss(avg_train_loss)\n",
    "\n",
    "    # ========== VALIDATION ==========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    bleu_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in val_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            inputs = captions[:, :-1]\n",
    "            targets = captions[:, 1:]\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(images, inputs)\n",
    "                loss = criterion(\n",
    "                    outputs.reshape(-1, outputs.size(-1)),\n",
    "                    targets.reshape(-1)\n",
    "                )\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_ppl = perplexity_from_loss(avg_val_loss)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), '/kaggle/working/image_caption_model_best.pth')\n",
    "        print(\"âœ… Best model saved!\")\n",
    "\n",
    "    print(f\"\\nðŸ“Š Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"   Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"   Train PPL:  {train_ppl:.2f}\")\n",
    "    print(f\"   Val Loss:   {avg_val_loss:.4f}\")\n",
    "    print(f\"   Val PPL:    {val_ppl:.2f}\")\n",
    "    print(f\"   LR:         {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11355675",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T10:14:55.482196Z",
     "iopub.status.busy": "2026-01-24T10:14:55.481893Z",
     "iopub.status.idle": "2026-01-24T10:14:55.689430Z",
     "shell.execute_reply": "2026-01-24T10:14:55.688803Z"
    },
    "papermill": {
     "duration": 0.215331,
     "end_time": "2026-01-24T10:14:55.691127",
     "exception": false,
     "start_time": "2026-01-24T10:14:55.475796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model after 15 epochs\n",
    "torch.save(model.state_dict(), '/kaggle/working/image_caption_model_after.pth')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 623289,
     "sourceId": 1111676,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 870709,
     "sourceId": 1483651,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2009.171666,
   "end_time": "2026-01-24T10:14:58.513983",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-24T09:41:29.342317",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
